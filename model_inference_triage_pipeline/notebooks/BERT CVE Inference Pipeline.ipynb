{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Text Processor Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample:\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "       When running eval/predict on the TPU, we need to pad the number of examples\n",
    "       to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "       size. \n",
    "       \n",
    "       The alternative is to drop the last batch, which is bad because it means\n",
    "       the entire output data won't be generated.\n",
    "       \n",
    "       We use this class instead of `None` because treating `None` as padding\n",
    "       batches could cause silent errors.\n",
    "       \n",
    "       Won't usually cause issues on CPU\\GPU hopefully.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "class InputExample:\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \n",
    "        \"\"\"Constructs a InputExample.\n",
    "        \n",
    "        Args:\n",
    "          guid: Unique id for the example.\n",
    "          text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "          text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "          label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as tf_hub\n",
    "from tqdm import tqdm\n",
    "import bert\n",
    "from bert import tokenization\n",
    "import numpy as np\n",
    "\n",
    "class BertTextProcessor:\n",
    "    \n",
    "    def __init__(self, tf_session, bert_model_path, max_seq_length=128):\n",
    "        self.tokenizer = None\n",
    "        self.bert_path = bert_model_path\n",
    "        self.tf_sess = tf_session\n",
    "        self.input_examples = []\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.input_ids = None\n",
    "        self.input_masks = None\n",
    "        self.segment_ids = None\n",
    "        self.labels = None\n",
    "        \n",
    "    \n",
    "    def create_bert_tokenizer(self):\n",
    "        \"\"\"Get the vocab file and casing info from \n",
    "           BERT tensorflow hub model.\"\"\"\n",
    "        print('Loading Base BERT Model')\n",
    "        bert_model =  tf_hub.Module(self.bert_path)\n",
    "        tokenization_info = bert_model(signature=\"tokenization_info\", as_dict=True)\n",
    "        vocab_file, do_lower_case = self.tf_sess.run(\n",
    "            [\n",
    "                tokenization_info[\"vocab_file\"],\n",
    "                tokenization_info[\"do_lower_case\"],\n",
    "            ]\n",
    "        )\n",
    "        print('Loading BERT WordPiece Tokenizer')\n",
    "        self.tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n",
    "                                                         do_lower_case=do_lower_case)\n",
    "        \n",
    "    \n",
    "    def convert_text_to_input_examples(self, texts, labels=[]):\n",
    "        \"\"\"Create InputExamples based on instances of the \n",
    "           bert.run_classifier.InputExample class\"\"\"\n",
    "        \n",
    "        labels = labels or [None] * len(texts)\n",
    "        print('Creating Input Examples from data')\n",
    "        for text, label in tqdm(zip(texts, labels), desc=\"Converting text to examples\"):\n",
    "            self.input_examples.append(\n",
    "                InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def convert_single_example(self, tokenizer, example, max_seq_length):\n",
    "        \"\"\"Converts a single example instance of class `InputExample` \n",
    "           into a single instance of features which consist of the following\n",
    "            - input_id\n",
    "            - input_mask\n",
    "            - segment_id\n",
    "            - label (None in case of inference)\n",
    "           this is based on instances of the `bert.run_classifier.InputFeatures`\n",
    "           class which is usually generated from the function \n",
    "           `bert.run_classifier.convert_single_example()\"\"\"\n",
    "        \n",
    "        if isinstance(example, PaddingInputExample):\n",
    "            input_ids = [0] * max_seq_length\n",
    "            input_mask = [0] * max_seq_length\n",
    "            segment_ids = [0] * max_seq_length\n",
    "            label = 0\n",
    "            return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        \n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        \n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "            \n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. \n",
    "        # Only real tokens are attended to in the attention layers.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "        \n",
    "        # double check lengths are alright\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        return input_ids, input_mask, segment_ids, example.label\n",
    "    \n",
    "    \n",
    "    def convert_examples_to_features(self):\n",
    "        \"\"\"Convert a set of `InputExample` instancess to a list \n",
    "           of instances of`InputFeatures` using the \n",
    "           convert_single_example(...) function.\"\"\"\n",
    "        \n",
    "        print('Creating BERT Input Features from Input Examples')\n",
    "        input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "        for example in tqdm(self.input_examples, desc=\"Converting examples to features\"):\n",
    "            input_id, input_mask, segment_id, label = self.convert_single_example(\n",
    "                self.tokenizer, example, self.max_seq_length\n",
    "            )\n",
    "            input_ids.append(input_id)\n",
    "            input_masks.append(input_mask)\n",
    "            segment_ids.append(segment_id)\n",
    "            labels.append(label)\n",
    "            \n",
    "        self.input_ids = np.array(input_ids)\n",
    "        self.input_masks = np.array(input_masks)\n",
    "        self.segment_ids = np.array(segment_ids)\n",
    "        self.labels = np.array(labels).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT CVE Classifier Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, bert_model_path, n_fine_tune_encoders=10, **kwargs,):\n",
    "        \n",
    "        self.n_fine_tune_encoders = n_fine_tune_encoders\n",
    "        self.trainable = True\n",
    "        # change only based on base bert output layer shape\n",
    "        self.output_size = 768\n",
    "        self.bert_path = bert_model_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        print('Loading Base BERT Model')\n",
    "        self.bert = tf_hub.Module(self.bert_path,\n",
    "                                  trainable=self.trainable, \n",
    "                                  name=f\"{self.name}_module\")\n",
    "\n",
    "        # Remove unused layers\n",
    "        # CLS layers cause an error if you try to tune them\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [var for var in trainable_vars \n",
    "                                  if not \"/cls/\" in var.name]\n",
    "        trainable_layers = [\"embeddings\", \"pooler/dense\"]\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        # we fine-tune all layers per encoder\n",
    "        # by default we tune all 10 encoders\n",
    "        for i in range(self.n_fine_tune_encoders+1):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(10 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [var for var in trainable_vars\n",
    "                                  if any([l in var.name \n",
    "                                              for l in trainable_layers])]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:# and 'encoder/layer' not in var.name:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        print('Trainable layers:', len(self._trainable_weights))\n",
    "        print('Non Trainable layers:', len(self._non_trainable_weights))\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print('Constructing Base BERT architecture')\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, \n",
    "                           input_mask=input_mask, \n",
    "                           segment_ids=segment_ids)\n",
    "        \n",
    "        pooled = self.bert(inputs=bert_inputs, \n",
    "                           signature=\"tokens\", \n",
    "                           as_dict=True)[\"pooled_output\"]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNotBuiltException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier:\n",
    "    \n",
    "    def __init__(self, bert_model_path, max_seq_length=128, \n",
    "                 n_fine_tune_encoders=10, model_weights_path=None):\n",
    "        self.bert_path = bert_model_path\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.n_fine_tune_encoders = n_fine_tune_encoders\n",
    "        self.model_estimator = None\n",
    "        self.model_weights_path = model_weights_path\n",
    "    \n",
    "    def build_model_architecture(self): \n",
    "        print('Build BERT Classifier CVE Model Architecture')\n",
    "        inp_id = tf.keras.layers.Input(shape=(self.max_seq_length,), \n",
    "                                       name=\"input_ids\")\n",
    "        inp_mask = tf.keras.layers.Input(shape=(self.max_seq_length,), \n",
    "                                         name=\"input_masks\")\n",
    "        inp_segment = tf.keras.layers.Input(shape=(self.max_seq_length,), \n",
    "                                            name=\"segment_ids\")\n",
    "        bert_inputs = [inp_id, inp_mask, inp_segment]\n",
    "\n",
    "        bert_output = BertLayer(bert_model_path=self.bert_path, \n",
    "                                n_fine_tune_encoders=self.n_fine_tune_encoders)(bert_inputs)\n",
    "\n",
    "        dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "        pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "        model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
    "                      metrics=['accuracy'])    \n",
    "        self.model_estimator = model\n",
    "        \n",
    "    \n",
    "    def load_model_weights(self, model_weights_path=None):\n",
    "        print('Loading BERT Classifier CVE Model Weights')\n",
    "        self.model_weights_path = model_weights_path or self.model_weights_path\n",
    "        if not self.model_estimator:\n",
    "            self.build_model_architecture()\n",
    "        self.model_estimator.load_weights(self.model_weights_path)\n",
    "            \n",
    "    \n",
    "    def get_model(self):\n",
    "        if not self.model_estimator:\n",
    "            raise ModelNotBuiltException(\n",
    "                \"BERT Classifier CVE Model doesn't exist. Please build model first\")\n",
    "        else:\n",
    "            return self.model_estimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(tf_session):\n",
    "    tf_session.run(tf.local_variables_initializer())\n",
    "    tf_session.run(tf.global_variables_initializer())\n",
    "    tf_session.run(tf.tables_initializer())\n",
    "    K.set_session(tf_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for bert model and tokenization\n",
    "BERT_PATH = \"models/model_assets/gokube-phase2/base_bert_tfhub_models/bert_uncased_L12_H768_A12\"\n",
    "BERT_CVE_MODEL_PATH = \"../../saved_models/bert_cve_model_weights_seq512b15.h5\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17432, 5811)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('../../data/GH_complete_labeled_issues_prs - preprocessed.csv', encoding='utf-8', \n",
    "                      na_filter=False)\n",
    "dataset = dataset[dataset.label != 0]\n",
    "texts = dataset['description'].tolist()\n",
    "labels = dataset['label'].tolist()\n",
    "labels = [0 if item == 1 else 1 for item in labels]\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(texts, labels, \n",
    "                                                                    test_size=0.25, \n",
    "                                                                    random_state=SEED) \n",
    "len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base BERT Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting text to examples: 100it [00:00, 267323.39it/s]\n",
      "Converting examples to features:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT WordPiece Tokenizer\n",
      "Creating Input Examples from data\n",
      "Creating BERT Input Features from Input Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 100/100 [00:02<00:00, 47.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build BERT Classifier CVE Model Architecture\n",
      "Loading Base BERT Model\n",
      "Trainable layers: 199\n",
      "Non Trainable layers: 5\n",
      "Constructing Base BERT architecture\n",
      "Loading BERT Classifier CVE Model Weights\n",
      "100/100 [==============================] - 8s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "initialize_vars(sess)\n",
    "\n",
    "# process text data\n",
    "btp = BertTextProcessor(tf_session=sess, \n",
    "                        bert_model_path=BERT_PATH, \n",
    "                        max_seq_length=MAX_SEQ_LENGTH)\n",
    "btp.create_bert_tokenizer()\n",
    "btp.convert_text_to_input_examples(test_text[:100])\n",
    "btp.convert_examples_to_features()\n",
    "\n",
    "# load pre-trained classification model\n",
    "bc = BERTClassifier(bert_model_path=BERT_PATH, \n",
    "                    max_seq_length=MAX_SEQ_LENGTH)\n",
    "bc.build_model_architecture()\n",
    "bc.load_model_weights(model_weights_path=BERT_CVE_MODEL_PATH)\n",
    "\n",
    "# perform model inference\n",
    "test_predictions = bc.model_estimator.predict(x=[btp.input_ids, \n",
    "                                                 btp.input_masks, \n",
    "                                                 btp.segment_ids],\n",
    "                                              batch_size=512,\n",
    "                                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        96\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.99      0.88      0.93       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n",
      "[[96  0]\n",
      " [ 1  3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_preds = test_predictions.ravel()\n",
    "test_preds = [1 if pred > 0.5 else 0 for pred in test_preds]\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true=test_labels[:100], y_pred=test_preds))\n",
    "print(confusion_matrix(y_true=test_labels[:100], y_pred=test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base BERT Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting text to examples: 100it [00:00, 165325.34it/s]\n",
      "Converting examples to features:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT WordPiece Tokenizer\n",
      "Creating Input Examples from data\n",
      "Creating BERT Input Features from Input Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 100/100 [00:02<00:00, 44.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build BERT Classifier CVE Model Architecture\n",
      "Loading Base BERT Model\n",
      "Trainable layers: 199\n",
      "Non Trainable layers: 5\n",
      "Constructing Base BERT architecture\n",
      "Loading BERT Classifier CVE Model Weights\n",
      "100/100 [==============================] - 419s 4s/step\n"
     ]
    }
   ],
   "source": [
    "with tf.device('cpu:0'):  \n",
    "    \n",
    "    # Initialize session\n",
    "    sess = tf.Session()\n",
    "    initialize_vars(sess)\n",
    "    \n",
    "    # process text data\n",
    "    btp = BertTextProcessor(tf_session=sess, \n",
    "                            bert_model_path=BERT_PATH, \n",
    "                            max_seq_length=MAX_SEQ_LENGTH)\n",
    "    \n",
    "    btp.create_bert_tokenizer()\n",
    "    btp.convert_text_to_input_examples(test_text[:100])\n",
    "    btp.convert_examples_to_features()\n",
    "    \n",
    "    # load pre-trained classification model\n",
    "    bc = BERTClassifier(bert_model_path=BERT_PATH, \n",
    "                    max_seq_length=MAX_SEQ_LENGTH)\n",
    "    bc.build_model_architecture()\n",
    "    bc.load_model_weights(model_weights_path=BERT_CVE_MODEL_PATH)\n",
    "    \n",
    "    # model inference\n",
    "    test_predictions = bc.model_estimator.predict(x=[btp.input_ids, \n",
    "                                                     btp.input_masks, \n",
    "                                                     btp.segment_ids],\n",
    "                                                  batch_size=512,\n",
    "                                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        96\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.99      0.88      0.93       100\n",
      "weighted avg       0.99      0.99      0.99       100\n",
      "\n",
      "[[96  0]\n",
      " [ 1  3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_preds = test_predictions.ravel()\n",
    "test_preds = [1 if pred > 0.5 else 0 for pred in test_preds]\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true=test_labels[:100], y_pred=test_preds))\n",
    "print(confusion_matrix(y_true=test_labels[:100], y_pred=test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
