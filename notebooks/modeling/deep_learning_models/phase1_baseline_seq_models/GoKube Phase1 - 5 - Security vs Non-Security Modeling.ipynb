{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import text_normalizer as tn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import dill\n",
    "import gc\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152151 entries, 0 to 152150\n",
      "Data columns (total 2 columns):\n",
      "description    152151 non-null object\n",
      "label          152151 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/gokube_phase1_jun19/GH_complete_labeled_issues_prs.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['description'].values\n",
    "labels = df['label'].values\n",
    "labels = np.array([0 if item == 0 else 1 for item in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing: starting\n",
      "ThreadPoolExecutor-0_0: working on doc num: 0\n",
      "ThreadPoolExecutor-0_2: working on doc num: 5000\n",
      "ThreadPoolExecutor-0_13: working on doc num: 10000\n",
      "ThreadPoolExecutor-0_14: working on doc num: 15000\n",
      "ThreadPoolExecutor-0_11: working on doc num: 20000\n",
      "ThreadPoolExecutor-0_0: working on doc num: 25000\n",
      "ThreadPoolExecutor-0_3: working on doc num: 30000\n",
      "ThreadPoolExecutor-0_3: working on doc num: 35000\n",
      "ThreadPoolExecutor-0_20: working on doc num: 40000\n",
      "ThreadPoolExecutor-0_28: working on doc num: 45000\n",
      "ThreadPoolExecutor-0_29: working on doc num: 50000\n",
      "ThreadPoolExecutor-0_23: working on doc num: 55000\n",
      "ThreadPoolExecutor-0_24: working on doc num: 60000\n",
      "ThreadPoolExecutor-0_5: working on doc num: 65000\n",
      "ThreadPoolExecutor-0_27: working on doc num: 70000\n",
      "ThreadPoolExecutor-0_26: working on doc num: 75000\n",
      "ThreadPoolExecutor-0_28: working on doc num: 80000\n",
      "ThreadPoolExecutor-0_5: working on doc num: 85000\n",
      "ThreadPoolExecutor-0_7: working on doc num: 90000\n",
      "ThreadPoolExecutor-0_24: working on doc num: 95000\n",
      "ThreadPoolExecutor-0_10: working on doc num: 100000\n",
      "ThreadPoolExecutor-0_5: working on doc num: 105000\n",
      "ThreadPoolExecutor-0_29: working on doc num: 110000\n",
      "ThreadPoolExecutor-0_0: working on doc num: 115000\n",
      "ThreadPoolExecutor-0_17: working on doc num: 120000\n",
      "ThreadPoolExecutor-0_15: working on doc num: 125000\n",
      "ThreadPoolExecutor-0_0: working on doc num: 130000\n",
      "ThreadPoolExecutor-0_4: working on doc num: 135000\n",
      "ThreadPoolExecutor-0_17: working on doc num: 140000\n",
      "ThreadPoolExecutor-0_20: working on doc num: 145000\n",
      "ThreadPoolExecutor-0_12: working on doc num: 150000\n",
      "ThreadPoolExecutor-0_9: working on doc num: 152150\n",
      "152151\n",
      "CPU times: user 16min 25s, sys: 37.6 s, total: 17min 2s\n",
      "Wall time: 16min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_docs = tn.pre_process_documents_parallel(documents=docs)\n",
    "print(len(norm_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/gokube_phase1_jun19/gh_preprocessed_descriptions.pkl', 'wb') as f:\n",
    "    dill.dump(norm_docs, f)\n",
    "    \n",
    "with open('./data/gokube_phase1_jun19/gh_labels.pkl', 'wb') as f:\n",
    "    dill.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/gokube_phase1_jun19/gh_preprocessed_descriptions.pkl', 'rb') as f:\n",
    "    norm_docs = dill.load(f)\n",
    "    \n",
    "with open('./data/gokube_phase1_jun19/gh_labels.pkl', 'rb') as f:\n",
    "    labels = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on 75:25 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114113, 38038)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(norm_docs, labels, \n",
    "                                                    test_size=0.25, random_state=SEED)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "EMBED_SIZE = 300 # how big is each word vector\n",
    "MAX_FEATURES = 800000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_LEN = 1000 # max number of words in a doc to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "CVE_WORD2IDX_MAP_FILE = 'models/v3-jun19/embeddings/security_tokenizer_word2idx.pkl'\n",
    "\n",
    "if not os.path.isfile(CVE_WORD2IDX_MAP_FILE):\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    tokenizer.word_index['<PAD>'] = 0\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'wb') as f:\n",
    "        dill.dump(tokenizer.word_index, f)\n",
    "else:\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "        word2idx = dill.load(f)\n",
    "    tokenizer.word_index = word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542585"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES = len(tokenizer.word_index)\n",
    "MAX_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "train_X = tokenizer.texts_to_sequences(X_train)\n",
    "test_X = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pad the sentences \n",
    "train_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=MAX_LEN)\n",
    "test_X = keras.preprocessing.sequence.pad_sequences(test_X, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112487, 1000), (112487,), (37468, 1000), (37468,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_lengths = np.array([len(np.nonzero(item)[0]) for item in train_X])\n",
    "train_X_idx = np.argwhere(train_X_lengths >= 5).ravel()\n",
    "train_X = train_X[train_X_idx]\n",
    "train_y = y_train[train_X_idx]\n",
    "\n",
    "test_X_lengths = np.array([len(np.nonzero(item)[0]) for item in test_X])\n",
    "test_X_idx = np.argwhere(test_X_lengths >= 5).ravel()\n",
    "test_X = test_X[test_X_idx]\n",
    "test_y = y_test[test_X_idx]\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):    \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542585, 300)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './models/v3-jun19/embeddings/security_model_fasttext_init_embeddings.pkl'\n",
    "\n",
    "if not os.path.isfile(FASTTEXT_INIT_EMBEDDINGS_FILE):\n",
    "    FASTTEXT_EMBEDDINGS_PATH = './embeddings/fasttext/crawl-300d-2M.vec'\n",
    "    ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=FASTTEXT_EMBEDDINGS_PATH)\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(ft_embeddings, f)\n",
    "else:\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        ft_embeddings = dill.load(f)\n",
    "        \n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542585, 300)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAM_INIT_EMBEDDINGS_FILE = './models/v3-jun19/embeddings/security_model_paragram_init_embeddings.pkl'\n",
    "\n",
    "if not os.path.isfile(PARAGRAM_INIT_EMBEDDINGS_FILE):\n",
    "    PARAGRAM_EMBEDDINGS_PATH = './embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    pg_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=PARAGRAM_EMBEDDINGS_PATH)\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(pg_embeddings, f)\n",
    "else:\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        pg_embeddings = dill.load(f)\n",
    "        \n",
    "pg_embeddings.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542585, 300)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pretrained_embeddings = np.mean([ft_embeddings, pg_embeddings], axis = 0)\n",
    "avg_pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = keras.regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = keras.constraints.get(W_constraint)\n",
    "        self.b_constraint = keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        # TF backend doesn't support it\n",
    "        # eij = K.dot(x, self.W) \n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), \n",
    "                              K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "def build_gru_model(embedding_matrix, embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size, \n",
    "                                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.CuDNNGRU(gru_units*2, return_sequences=True))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.CuDNNGRU(gru_units, return_sequences=True))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units*2, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    # initialize the model\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    # make the model parallel\n",
    "    #model = multi_gpu_model(model, gpus=2)\n",
    "       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         162775500 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000, 64)          31104     \n",
      "_________________________________________________________________\n",
      "attention_layer_1 (Attention (None, 64)                1064      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 162,954,485\n",
      "Trainable params: 162,954,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = build_gru_model(embedding_matrix=avg_pretrained_embeddings, embedding_size=EMBED_SIZE, \n",
    "                            max_len=MAX_LEN, max_features=MAX_FEATURES, gru_units=32)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5916128396035698, 1: 6.457750269106566}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_y),\n",
    "                                                 train_y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights[1] *= 2\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4,\n",
    "                              patience=2, min_lr=0.00001)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, \n",
    "                           mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "callbacks = [reduce_lr, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 162775500 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 107986 samples, validate on 11999 samples\n",
      "Epoch 1/10\n",
      "107986/107986 [==============================] - 397s 4ms/step - loss: 0.4863 - acc: 0.7466 - val_loss: 0.1825 - val_acc: 0.9596\n",
      "Epoch 2/10\n",
      "107986/107986 [==============================] - 370s 3ms/step - loss: 0.1581 - acc: 0.9655 - val_loss: 0.1644 - val_acc: 0.9717\n",
      "Epoch 3/10\n",
      "107986/107986 [==============================] - 369s 3ms/step - loss: 0.0812 - acc: 0.9815 - val_loss: 0.1777 - val_acc: 0.9579\n",
      "Epoch 4/10\n",
      "107986/107986 [==============================] - 369s 3ms/step - loss: 0.0408 - acc: 0.9893 - val_loss: 0.2361 - val_acc: 0.9681\n",
      "Epoch 5/10\n",
      "107986/107986 [==============================] - 369s 3ms/step - loss: 0.0197 - acc: 0.9941 - val_loss: 0.2888 - val_acc: 0.9673\n",
      "Epoch 6/10\n",
      "107986/107986 [==============================] - 373s 3ms/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.3896 - val_acc: 0.9694\n"
     ]
    }
   ],
   "source": [
    "history = gru_model.fit(train_X, train_y, batch_size=256, epochs=10, callbacks=callbacks,\n",
    "                        class_weight=class_weights, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.save('./models/v3-jun19/model_files/security_model_train75-jun19.h5')\n",
    "gru_model.save_weights('./models/v3-jun19/model_files/security_model_train75-jun19_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 162775500 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37468/37468 [==============================] - 38s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "gru_model = keras.models.load_model('./models/v3-jun19/model_files/security_model_train75-jun19.h5',\n",
    "                                        custom_objects={'AttentionLayer': AttentionLayer})\n",
    "pred_y = gru_model.predict([test_X], batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 0.5 else 0 for prob in pred_yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31308,   369],\n",
       "       [  353,  5438]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "confusion_matrix(y_true=test_y, y_pred=pred_yl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     31677\n",
      "          1       0.94      0.94      0.94      5791\n",
      "\n",
      "avg / total       0.98      0.98      0.98     37468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=test_y, y_pred=pred_yl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Full Data now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152135, 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(norm_docs, labels, \n",
    "                                                    test_size=0.0001, random_state=SEED)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "CVE_WORD2IDX_MAP_FILE = 'models/v3-jun19/embeddings/security_tokenizer_word2idx_fulldata.pkl'\n",
    "\n",
    "if not os.path.isfile(CVE_WORD2IDX_MAP_FILE):\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    tokenizer.word_index['<PAD>'] = 0\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'wb') as f:\n",
    "        dill.dump(tokenizer.word_index, f)\n",
    "else:\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "    with open(CVE_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "        word2idx = dill.load(f)\n",
    "    tokenizer.word_index = word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649383"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES = len(tokenizer.word_index)\n",
    "MAX_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149939, 1000), (149939,), (16, 1000), (16,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = tokenizer.texts_to_sequences(X_train)\n",
    "test_X = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "train_X = keras.preprocessing.sequence.pad_sequences(train_X, maxlen=MAX_LEN)\n",
    "test_X = keras.preprocessing.sequence.pad_sequences(test_X, maxlen=MAX_LEN)\n",
    "\n",
    "train_X_lengths = np.array([len(np.nonzero(item)[0]) for item in train_X])\n",
    "train_X_idx = np.argwhere(train_X_lengths >= 5).ravel()\n",
    "train_X = train_X[train_X_idx]\n",
    "train_y = y_train[train_X_idx]\n",
    "\n",
    "test_X_lengths = np.array([len(np.nonzero(item)[0]) for item in test_X])\n",
    "test_X_idx = np.argwhere(test_X_lengths >= 5).ravel()\n",
    "test_X = test_X[test_X_idx]\n",
    "test_y = y_test[test_X_idx]\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT Embeddings shape: (649383, 300)\n",
      "PG Embeddings shape: (649383, 300)\n",
      "AVG Embeddings shape: (649383, 300)\n"
     ]
    }
   ],
   "source": [
    "FASTTEXT_INIT_EMBEDDINGS_FILE = './models/v3-jun19/embeddings/security_model_fasttext_init_embeddings_fulldata.pkl'\n",
    "\n",
    "if not os.path.isfile(FASTTEXT_INIT_EMBEDDINGS_FILE):\n",
    "    FASTTEXT_EMBEDDINGS_PATH = './embeddings/fasttext/crawl-300d-2M.vec'\n",
    "    ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=FASTTEXT_EMBEDDINGS_PATH)\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(ft_embeddings, f)\n",
    "else:\n",
    "    with open(FASTTEXT_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        ft_embeddings = dill.load(f)\n",
    "        \n",
    "\n",
    "PARAGRAM_INIT_EMBEDDINGS_FILE = './models/v3-jun19/embeddings/security_model_paragram_init_embeddings_fulldata.pkl'\n",
    "\n",
    "if not os.path.isfile(PARAGRAM_INIT_EMBEDDINGS_FILE):\n",
    "    PARAGRAM_EMBEDDINGS_PATH = './embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    pg_embeddings = load_pretrained_embeddings(word_to_index=word2idx, max_features=MAX_FEATURES, \n",
    "                                               embedding_size=EMBED_SIZE, \n",
    "                                               embedding_file_path=PARAGRAM_EMBEDDINGS_PATH)\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        dill.dump(pg_embeddings, f)\n",
    "else:\n",
    "    with open(PARAGRAM_INIT_EMBEDDINGS_FILE, 'rb') as f:\n",
    "        pg_embeddings = dill.load(f)\n",
    "        \n",
    "avg_pretrained_embeddings = np.mean([ft_embeddings, pg_embeddings], axis = 0)\n",
    "\n",
    "print('FT Embeddings shape:', ft_embeddings.shape)\n",
    "print('PG Embeddings shape:', pg_embeddings.shape)\n",
    "print('AVG Embeddings shape:', avg_pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         194814900 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 1000, 64)          31104     \n",
      "_________________________________________________________________\n",
      "attention_layer_1 (Attention (None, 64)                1064      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 194,993,885\n",
      "Trainable params: 194,993,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model = build_gru_model(embedding_matrix=avg_pretrained_embeddings, embedding_size=EMBED_SIZE, \n",
    "                            max_len=MAX_LEN, max_features=MAX_FEATURES, gru_units=32)\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5914660123706135, 1: 6.466511407254065}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_y),\n",
    "                                                 train_y)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights[1] *= 2\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 194814900 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 134945 samples, validate on 14994 samples\n",
      "Epoch 1/10\n",
      "134945/134945 [==============================] - 497s 4ms/step - loss: 0.4891 - acc: 0.6904 - val_loss: 0.1849 - val_acc: 0.9540\n",
      "Epoch 2/10\n",
      "134945/134945 [==============================] - 490s 4ms/step - loss: 0.1442 - acc: 0.9699 - val_loss: 0.1590 - val_acc: 0.9772\n",
      "Epoch 3/10\n",
      "134945/134945 [==============================] - 482s 4ms/step - loss: 0.0696 - acc: 0.9835 - val_loss: 0.2089 - val_acc: 0.9586\n",
      "Epoch 4/10\n",
      "134945/134945 [==============================] - 483s 4ms/step - loss: 0.0349 - acc: 0.9904 - val_loss: 0.3053 - val_acc: 0.9707\n",
      "Epoch 5/10\n",
      "134945/134945 [==============================] - 490s 4ms/step - loss: 0.0159 - acc: 0.9958 - val_loss: 0.4021 - val_acc: 0.9706\n",
      "Epoch 6/10\n",
      "134945/134945 [==============================] - 490s 4ms/step - loss: 0.0104 - acc: 0.9975 - val_loss: 0.4573 - val_acc: 0.9711\n",
      "Epoch 7/10\n",
      "134945/134945 [==============================] - 489s 4ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.5326 - val_acc: 0.9727\n",
      "Epoch 8/10\n",
      "134945/134945 [==============================] - 481s 4ms/step - loss: 0.0056 - acc: 0.9986 - val_loss: 0.5195 - val_acc: 0.9713\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.4,\n",
    "                              patience=2, min_lr=0.00001)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, \n",
    "                           mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "callbacks = [reduce_lr, early_stop]\n",
    "\n",
    "history = gru_model.fit(train_X, train_y, batch_size=256, epochs=10, callbacks=callbacks,\n",
    "                        class_weight=class_weights, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model.save('./models/v3-jun19/model_files/security_model_train99-jun19.h5')\n",
    "gru_model.save_weights('./models/v3-jun19/model_files/security_model_train99-jun19_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149939/149939 [==============================] - 148s 988us/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = gru_model.predict([train_X], batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 0.4 else 0 for prob in pred_yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126311,    441],\n",
       "       [   161,  23026]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=train_y, y_pred=pred_yl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_cpumodel(embedding_size, max_len, max_features, gru_units=32):\n",
    "    \n",
    "    inp = keras.layers.Input(shape=(max_len,))\n",
    "    x = keras.layers.Embedding(max_features, embedding_size, trainable=True)(inp)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units*2, return_sequences=True, \n",
    "                                                    reset_after=True, recurrent_activation='sigmoid'))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.GRU(gru_units, return_sequences=True, reset_after=True, \n",
    "                                                    recurrent_activation='sigmoid'))(x)\n",
    "    x = AttentionLayer(max_len)(x)\n",
    "    x = keras.layers.Dense(gru_units*2, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = keras.layers.Dense(gru_units, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(rate=0.2)(x)\n",
    "    outp = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=inp, outputs=outp)       \n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1000, 300)         194814900 \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 1000, 128)         140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 1000, 64)          31104     \n",
      "_________________________________________________________________\n",
      "attention_layer_3 (Attention (None, 64)                1064      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 194,993,885\n",
      "Trainable params: 194,993,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CVE_WORD2IDX_MAP_FILE = 'models/v3-jun19/embeddings/security_tokenizer_word2idx_fulldata.pkl'\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<UNK>', num_words=MAX_FEATURES+1)\n",
    "with open(CVE_WORD2IDX_MAP_FILE, 'rb') as f:\n",
    "    word2idx = dill.load(f)\n",
    "    tokenizer.word_index = word2idx\n",
    "MAX_FEATURES = len(tokenizer.word_index)\n",
    "EMBED_SIZE = 300\n",
    "MAX_LEN = 1000\n",
    "\n",
    "\n",
    "gru_cpu_model = build_gru_cpumodel(embedding_size=EMBED_SIZE,\n",
    "                                   max_len=MAX_LEN, max_features=MAX_FEATURES, gru_units=32)\n",
    "gru_cpu_model.load_weights('./models/v3-jun19/model_files/security_model_train99-jun19_weights.h5')\n",
    "gru_cpu_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149939/149939 [==============================] - 146s 971us/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = gru_model.predict([train_X], batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yr = pred_y.ravel()\n",
    "pred_yl = [1 if prob > 0.4 else 0 for prob in pred_yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[126311,    441],\n",
       "       [   161,  23026]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true=train_y, y_pred=pred_yl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
