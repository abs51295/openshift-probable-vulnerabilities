{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 08:08:28.935521 139808272750400 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import text_normalizer as tn\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152151 entries, 0 to 152150\n",
      "Data columns (total 2 columns):\n",
      "description    152151 non-null object\n",
      "label          152151 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./data/GH_complete_labeled_issues_prs - preprocessed.csv', encoding='utf-8', \n",
    "                      na_filter=False)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset.label != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({1: 22572, 2: 671})\n",
      "After: Counter({0: 22572, 1: 671})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "texts = dataset['description'].tolist()\n",
    "labels = dataset['label'].tolist()\n",
    "\n",
    "print('Before:', Counter(labels))\n",
    "labels = [0 if item == 1 else 1 for item in labels]\n",
    "print('After:', Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17432, 5811)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(texts, labels, \n",
    "                                                                    test_size=0.25, random_state=SEED)\n",
    "len(train_text), len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "       When running eval/predict on the TPU, we need to pad the number of examples\n",
    "       to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "       size. The alternative is to drop the last batch, which is bad because it means\n",
    "       the entire output data won't be generated.\n",
    "       We use this class instead of `None` because treating `None` as padding\n",
    "       batches could cause silent errors.\n",
    "  \"\"\"\n",
    "    \n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  tf_hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "MAX_SEQ_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 08:08:34.764133 139808272750400 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_path=BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_lengths = np.array([len(doc.split(' ')) for doc in train_text])\n",
    "test_text_lengths = np.array([len(doc.split(' ')) for doc in test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17389, 17389, 5794, 5794)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_idx = np.argwhere(train_text_lengths >= 5).ravel()\n",
    "test_text_idx = np.argwhere(test_text_lengths >= 5).ravel()\n",
    "\n",
    "train_text = [train_text[i] for i in train_text_idx]\n",
    "train_labels = [train_labels[i] for i in train_text_idx]\n",
    "test_text = [test_text[i] for i in test_text_idx]\n",
    "test_labels = [test_labels[i] for i in test_text_idx]\n",
    "\n",
    "len(train_text), len(train_labels), len(test_text), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_labels)\n",
    "test_examples = convert_text_to_examples(test_text, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b37ebdfe971465fb3abb2f401c8d60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=5794, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#(train_input_ids, train_input_masks, \n",
    "# train_segment_ids, train_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "#                                                                  examples=train_examples, \n",
    "#                                                                  max_seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "(test_input_ids, test_input_masks, \n",
    " test_segment_ids, test_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                                examples=test_examples, \n",
    "                                                                max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5794, 512)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change from tf.layers to tf.keras\n",
    "class BertLayer(tf.layers.Layer):\n",
    "    \n",
    "    def __init__(self, bert_path, n_fine_tune_encoders=10, **kwargs,):\n",
    "        \n",
    "        self.n_fine_tune_encoders = n_fine_tune_encoders\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.bert_path = bert_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.bert = tf_hub.Module(self.bert_path,\n",
    "                                  trainable=self.trainable, \n",
    "                                  name=f\"{self.name}_module\")\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [var for var in trainable_vars \n",
    "                                  if not \"/cls/\" in var.name]\n",
    "        trainable_layers = [\"embeddings\", \"pooler/dense\"]\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_encoders+1):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(10 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [var for var in trainable_vars\n",
    "                                  if any([l in var.name \n",
    "                                              for l in trainable_layers])]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:# and 'encoder/layer' not in var.name:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        print('Trainable layers:', len(self._trainable_weights))\n",
    "        print('Non Trainable layers:', len(self._non_trainable_weights))\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, \n",
    "                           input_mask=input_mask, \n",
    "                           segment_ids=segment_ids)\n",
    "        \n",
    "        pooled = self.bert(inputs=bert_inputs, \n",
    "                           signature=\"tokens\", \n",
    "                           as_dict=True)[\"pooled_output\"]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(bert_path, max_seq_length, n_fine_tune_encoders=10): \n",
    "    \n",
    "    inp_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    inp_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    inp_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [inp_id, inp_mask, inp_segment]\n",
    "    \n",
    "    bert_output = BertLayer(bert_path=bert_path, \n",
    "                            n_fine_tune_encoders=n_fine_tune_encoders)(bert_inputs)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
    "                  metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 08:10:23.604956 139808272750400 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers: 199\n",
      "Non Trainable layers: 5\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0709 08:10:25.335416 139808272750400 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "with tf.device('cpu:0'):\n",
    "    model = build_model(bert_path=BERT_PATH, max_seq_length=MAX_SEQ_LENGTH, n_fine_tune_encoders=10)\n",
    "    initialize_vars(sess)\n",
    "    model.load_weights('./bert_cve_model_weights_seq512b15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4150s 4s/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(x=[test_input_ids[:1000], \n",
    "                                    test_input_masks[:1000], \n",
    "                                    test_segment_ids[:1000]],\n",
    "                                 batch_size=512,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4213s 4s/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(x=[test_input_ids[:1000], \n",
    "                                    test_input_masks[:1000], \n",
    "                                    test_segment_ids[:1000]],\n",
    "                                 batch_size=50,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT as a feature extractor - contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f26bda34e10>,\n",
       " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f26bda346d8>,\n",
       " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f26bda34748>,\n",
       " <__main__.BertLayer at 0x7f26bda34d68>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f27a07ace48>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f230551be48>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.models.Model(inputs=model.inputs, \n",
    "                          outputs=model.layers[3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.47619382, -0.6549353 , -0.9999619 ,  0.9375201 ,  0.97283417,\n",
       "         -0.78681475,  0.8963109 ,  0.52210534, -0.99991065,  0.9983653 ,\n",
       "         -0.9894177 ,  0.9995566 , -0.9802466 ,  0.99943465, -0.9486656 ,\n",
       "         -0.9768426 , -0.9933897 , -0.28235286,  0.5027091 , -0.88477266,\n",
       "          0.12394645,  0.9999988 , -0.98896664,  0.4716109 ,  0.27310258,\n",
       "          0.99990416, -0.8426162 , -0.9560043 , -0.9793524 , -0.88095266,\n",
       "         -0.7740885 ,  0.61384785,  0.99644727, -0.16334432, -0.9998966 ,\n",
       "          0.9947635 ,  0.72072214,  0.925317  , -0.74917775, -0.4458454 ,\n",
       "          0.9307592 ,  0.5275633 , -0.97903585,  0.94861597, -0.08209433,\n",
       "          0.03902055, -0.99998385,  0.7135445 ,  0.96231747,  0.99993336,\n",
       "          0.99981123,  0.9997307 ,  0.5342699 ,  0.56266433,  0.48588434,\n",
       "         -0.7462636 ,  0.46644196,  0.7054201 , -0.5018115 , -0.6332456 ,\n",
       "         -0.2952568 ,  0.78966975, -0.9981369 ,  0.9777408 ,  0.9999815 ,\n",
       "          0.9999764 , -0.89810616, -0.74434423, -0.7419038 , -0.17901136,\n",
       "          0.52780515,  0.64338803, -0.9360391 ,  0.8067022 ,  0.999851  ,\n",
       "          0.46597955,  0.3330149 ,  1.        , -0.9456624 ,  0.99446315,\n",
       "          0.9999037 ,  0.99986094, -0.04796609, -0.99979144,  0.9996032 ,\n",
       "         -0.9999989 ,  0.93954796, -0.4548903 ,  0.99827677,  0.60729474,\n",
       "          0.02183531, -0.72262305,  0.9999157 , -0.50246954, -0.9961772 ,\n",
       "         -0.89961   , -0.52256   , -0.9988975 , -0.7675964 , -0.6658072 ,\n",
       "          0.6426331 , -0.5101172 , -0.88202536, -0.63751084,  0.19533233,\n",
       "         -0.57493025, -0.25351036,  0.95024115,  0.9978036 ,  0.7022361 ,\n",
       "         -0.1042583 , -0.63221484,  0.7607252 ,  0.9285777 ,  0.06812141,\n",
       "         -0.48877862,  0.9932308 , -0.00760313,  0.99742484, -0.5883831 ,\n",
       "         -0.93952346, -0.71679443, -0.96259516, -0.9987221 ,  0.64523643,\n",
       "         -0.694905  , -0.99995565, -1.        , -0.9776287 , -0.9717981 ,\n",
       "         -0.97915   , -0.86426765,  0.99284923,  0.99118453,  0.59021634,\n",
       "         -0.9774181 ,  0.5175046 , -0.9887434 , -0.7113063 , -0.94493127,\n",
       "         -0.9976767 , -0.9914345 ,  0.9986168 , -0.54165745,  0.9725338 ,\n",
       "          0.917414  , -0.8690888 ,  0.19193806, -0.87404555,  0.9405058 ,\n",
       "         -0.9526165 , -0.7245738 , -0.9995063 ,  0.98710346,  0.2740983 ,\n",
       "         -0.9809391 , -0.9983477 , -0.9999745 , -0.9994559 , -0.60424984,\n",
       "         -0.84991616, -0.7827587 ,  0.9848064 ,  0.04198423, -0.98511153,\n",
       "          0.40797818,  0.9980443 ,  0.13858312,  0.6839365 , -0.98707247,\n",
       "          0.44300267, -0.6264792 , -0.9999374 ,  0.9805384 , -0.16046897,\n",
       "         -0.48278943, -0.9855417 , -0.7216175 ,  0.6225285 ,  0.9990619 ,\n",
       "         -0.11079814,  0.9641549 ,  0.9532214 , -0.9954588 , -0.6144988 ,\n",
       "          0.35331798, -0.69971204,  0.9831531 ,  0.8602244 ,  0.9856154 ,\n",
       "          0.21726821, -0.9894521 ,  0.96655804, -0.55025244, -0.17541783,\n",
       "         -0.60605484, -0.9848603 , -0.54714566, -0.05311852,  0.25332248,\n",
       "         -0.7881184 , -0.9655265 ,  0.9408304 , -0.8598114 ,  0.99976355,\n",
       "          0.07056781,  0.88652706, -0.8793436 ,  0.5549917 ,  0.7240046 ,\n",
       "          0.8390247 , -0.98994845, -0.9548833 , -0.5846801 ,  0.93391514,\n",
       "          0.9552073 ,  0.1950544 ,  0.8253423 , -0.67324895,  0.07343369,\n",
       "          0.9165527 , -0.84504986,  0.9995751 ,  0.75915414, -0.8422125 ,\n",
       "         -0.47803012,  0.69866365, -0.3483225 ,  0.6825401 ,  0.35011008,\n",
       "         -0.95060223,  0.9999192 , -0.4379177 ,  0.99261916, -0.7513576 ,\n",
       "         -0.99997044,  0.86517227,  0.9229039 , -0.7704295 , -0.77415556,\n",
       "         -0.4786161 , -0.99694586,  0.9980439 ,  0.970505  ,  0.9706877 ,\n",
       "         -0.9997536 ,  0.4828981 , -0.9998557 , -0.6316    ,  0.99956036,\n",
       "          0.9999512 , -0.5086552 ,  0.31471494, -0.99794215, -0.37257233,\n",
       "          0.9200244 ,  0.714062  ,  0.9643559 ,  0.9565459 , -0.9715691 ,\n",
       "          0.99568874,  0.9986301 ,  0.2144714 ,  0.995893  , -0.5621051 ,\n",
       "         -0.5750176 ,  0.9703077 ,  0.6809609 ,  0.6295321 ,  0.5902931 ,\n",
       "         -0.8169575 , -0.9376583 , -0.9387105 ,  0.9738375 ,  0.44401175,\n",
       "         -0.6921603 , -0.99990666,  0.52535135,  0.9725952 ,  0.49886236,\n",
       "         -0.05683964,  0.4750926 , -0.9999715 ,  0.950055  ,  0.99999964,\n",
       "         -0.9958742 , -0.93153065, -0.01955821, -0.99997854,  0.2934146 ,\n",
       "         -0.992887  , -0.99997085, -0.9999998 ,  0.9551487 , -0.97205555,\n",
       "         -0.5527568 , -1.        , -0.5179363 , -0.27795827,  0.97929776,\n",
       "          0.9993534 , -0.9942607 ,  0.7793735 , -1.        , -0.99902785,\n",
       "         -0.9787999 ,  0.0637413 ,  0.9997947 , -0.74885124, -0.9837592 ,\n",
       "          0.9869675 , -0.7070333 , -0.24877529,  0.46891257, -0.99996334,\n",
       "          0.08775397, -0.9998278 , -0.99923265,  0.9999762 ,  0.29812983,\n",
       "         -0.77990454,  0.9632643 , -0.13069546, -0.833185  ,  0.9637683 ,\n",
       "          0.9835323 , -0.6398514 ,  0.99752605,  0.89260054,  0.5183311 ,\n",
       "          0.64674395,  0.6216564 ,  0.7106278 ,  0.99895674, -0.00483619,\n",
       "         -0.6667512 ,  0.9684185 ,  0.29558277, -0.9433032 ,  0.98483425,\n",
       "         -0.9997201 ,  0.9810163 , -0.98288864,  0.2074732 ,  0.9997834 ,\n",
       "          1.        , -0.3297939 ,  0.8568706 ,  0.9761715 ,  0.70789   ,\n",
       "         -0.8869233 ,  1.        ,  0.8643012 ,  0.9909367 ,  0.06919049,\n",
       "          0.92722785, -0.6234022 , -0.5526566 , -0.99948907, -0.63377947,\n",
       "         -0.9997685 , -0.99828273, -0.99691427,  0.996217  ,  0.99996287,\n",
       "          0.811403  ,  0.9879539 , -0.99195975, -0.9505939 , -0.9758188 ,\n",
       "          0.9807625 ,  0.83151567, -0.99954945,  0.61882985,  0.8854775 ,\n",
       "          0.8523218 ,  0.8772385 , -0.446624  , -0.9008495 , -0.5538093 ,\n",
       "          0.68376976,  0.46350664, -0.99257255, -0.99541   ,  0.99969405,\n",
       "          0.69324315, -0.13659824,  0.6332455 , -0.44957617,  0.94628066,\n",
       "          0.9753689 ,  0.91988593,  1.        , -0.93882227,  0.99211   ,\n",
       "         -0.9944708 ,  0.09562358,  0.7872003 ,  0.6901531 ,  0.29743364,\n",
       "         -0.30906942,  0.91947275,  0.9944926 , -0.98059404,  0.996916  ,\n",
       "          0.1566023 ,  0.47442362, -0.53854096,  0.9998917 ,  0.9756114 ,\n",
       "          0.43495858,  0.98693985,  0.9999664 ,  0.6820059 ,  0.5395837 ,\n",
       "          0.9999945 , -0.988222  , -0.65790886,  0.32649314, -0.3567037 ,\n",
       "         -0.999971  , -0.6272225 , -0.51138365,  0.6633181 ,  0.97055906,\n",
       "         -0.6612361 ,  0.9741774 , -0.98742205,  0.99993354,  0.72385734,\n",
       "          0.72523355,  0.99625504,  1.        ,  0.97672164,  0.80599433,\n",
       "         -0.99381834,  0.9854111 , -0.99999505, -0.44871634, -0.47762868,\n",
       "         -0.42778945, -0.99997115, -0.5412018 ,  0.3714046 ,  0.99211264,\n",
       "          0.99988455,  0.99409133, -0.98979205,  0.99778986, -0.99472344,\n",
       "          0.9634154 ,  0.635435  , -0.9984583 , -0.8551303 ,  0.8928406 ,\n",
       "          0.99263626, -0.4582806 ,  0.9748037 , -0.99975526, -0.67015153,\n",
       "          0.5233276 , -0.11893801, -0.40892202,  0.9999006 , -0.99946785,\n",
       "         -0.98839706, -0.88120687, -0.06464861,  0.27228755,  0.88712466,\n",
       "         -0.53237855, -0.99998075, -0.6894964 ,  1.        , -0.48993313,\n",
       "          0.9997524 , -0.43586326,  0.920504  , -0.5512566 ,  0.4604656 ,\n",
       "          0.99982876,  0.75527084, -0.9998742 , -0.9999411 , -0.99571735,\n",
       "         -0.7073951 ,  0.4650167 ,  0.99200785,  0.99971735, -0.9318647 ,\n",
       "          0.36717832,  0.65634555, -0.8436304 ,  0.76790977,  0.98051006,\n",
       "         -0.58343637, -0.6893407 ,  0.4946023 , -0.40568745, -0.57139736,\n",
       "         -0.9942061 ,  1.        ,  0.39185652,  0.9299191 ,  0.99364144,\n",
       "         -0.9997406 ,  0.64641446,  0.9999999 , -0.73293686,  0.99588764,\n",
       "          0.7104108 ,  0.9302042 , -0.24934947,  0.9474475 , -0.35900888,\n",
       "         -0.7819231 ,  0.76671547,  0.6921572 , -0.9900893 , -0.67291677,\n",
       "          0.9847658 ,  0.15331636,  0.10575607,  0.988227  ,  0.9999956 ,\n",
       "         -0.36543298, -0.65066475,  0.23271449, -0.8824162 ,  0.9966878 ,\n",
       "          0.6087244 ,  0.98832667, -0.39728442,  0.7847941 , -0.9919026 ,\n",
       "          0.41037813, -0.43423444,  0.7206402 ,  0.99985707,  0.99456394,\n",
       "         -0.9999451 ,  0.98897815, -0.9640681 ,  0.9460383 ,  0.8282746 ,\n",
       "          0.99999815,  0.6520903 ,  0.9983298 ,  0.10445084, -0.23331048,\n",
       "          0.44852224, -0.95054764,  0.87883985,  0.98462623, -0.4751789 ,\n",
       "         -0.42740875,  0.6120663 , -0.63766515, -0.8242202 , -0.9369837 ,\n",
       "          0.74083567,  0.48751196, -0.7233572 , -0.5269713 ,  0.60979354,\n",
       "          0.67115366, -0.8829922 , -0.41159853,  0.77586997, -0.47174534,\n",
       "          0.6186882 , -0.5627947 , -0.393053  , -0.9999946 , -0.26381317,\n",
       "         -1.        ,  0.98964953,  0.999352  , -0.5607798 , -0.8400242 ,\n",
       "         -0.45390633,  0.9174114 ,  0.7223942 , -0.999964  , -0.9998115 ,\n",
       "         -0.20095035, -0.80874133, -0.9937099 ,  0.74251467,  0.870329  ,\n",
       "         -0.8177878 ,  0.14549887, -0.9968991 , -0.72967845, -0.7333461 ,\n",
       "          0.99999994,  0.80507904, -0.92944485, -0.9098173 ,  0.6173615 ,\n",
       "         -0.30161062,  1.        , -0.86336803,  0.98117936,  0.84496635,\n",
       "         -0.7675027 ,  0.5358676 ,  0.32672876,  0.2774106 , -0.58124524,\n",
       "         -0.99992967, -0.8825433 ,  0.984272  ,  0.05050961,  0.847943  ,\n",
       "         -0.7515501 , -0.6463633 ,  0.7862585 ,  0.99959993, -0.99692583,\n",
       "          0.4219177 , -0.00433816, -0.73984236, -0.8942803 , -0.98782426,\n",
       "          0.7473501 ,  0.5964084 ,  0.19213925,  0.99999976,  0.7753326 ,\n",
       "          0.9005811 , -0.90065455,  0.97505414, -0.647399  ,  0.94480366,\n",
       "          0.70146596,  0.66276854, -0.834086  , -0.65761876, -0.89182764,\n",
       "         -0.9998089 ,  0.8688648 , -0.9872824 , -0.9997608 ,  0.23124304,\n",
       "          0.9519381 ,  0.99491817,  0.99774224,  0.959048  , -0.03622879,\n",
       "         -0.66105866,  0.02463979,  0.27757064,  0.34747586,  0.4138575 ,\n",
       "         -0.9999999 , -0.9306219 ,  0.32874227,  0.99987054, -0.99478346,\n",
       "          0.98542124,  0.7109329 , -0.32148063,  0.9938292 , -0.70983964,\n",
       "         -0.40148368,  0.21292847,  0.54672647, -0.28297794, -0.66684955,\n",
       "          0.13517363, -0.30647525,  0.3646878 , -0.9998995 ,  0.70554245,\n",
       "          0.9979416 ,  0.62204796, -0.9994214 , -0.9393596 , -0.97949415,\n",
       "          0.99610204, -0.3010743 , -0.9989806 , -0.9957973 ,  0.49324197,\n",
       "         -0.9426214 ,  0.70795417,  0.72401327, -0.8398992 , -0.97423804,\n",
       "          0.03950142, -0.9968564 , -0.99981654,  0.01445015, -0.99932307,\n",
       "          0.6199958 , -0.9897426 ,  0.9641697 ,  0.69970506,  0.83975375,\n",
       "         -0.8862321 ,  0.34425887, -0.729279  , -0.6132844 , -0.9878655 ,\n",
       "         -0.6059001 ,  0.8176941 , -0.69358313, -0.76674986, -0.17817038,\n",
       "         -0.4008822 ,  0.7515672 , -0.9804378 , -0.6406152 ,  0.9459098 ,\n",
       "         -0.9079195 ,  0.9863624 , -0.6035895 , -0.7279112 , -0.61902595,\n",
       "         -0.99987936,  0.90404177,  0.7149454 , -0.7165296 , -0.9988347 ,\n",
       "          0.99905264,  0.51291025, -0.97200596,  0.99821967, -0.36187935,\n",
       "         -0.26889428,  0.10781013, -0.9430361 ,  0.82796746, -0.57767075,\n",
       "         -0.85931516, -0.8786524 ,  0.48120773,  0.99999595, -0.99974966,\n",
       "         -0.99996513,  0.24608341, -0.5908715 ,  0.65448755, -0.6299562 ,\n",
       "         -1.        ,  0.63786745, -0.9841978 ,  0.99852663, -0.9972349 ,\n",
       "          0.9994996 , -0.9944656 ,  0.4272403 , -0.7415489 , -0.31598106,\n",
       "          0.9984867 , -0.63541305, -0.9593179 , -0.63736194, -0.98164314,\n",
       "          0.9999329 , -0.90421546, -0.37017572, -0.99891776, -0.5345201 ,\n",
       "         -0.9967845 , -0.19088738, -0.8289598 ]], dtype=float32), (1, 768))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = m.predict(x=[test_input_ids[:1], \n",
    "             test_input_masks[:1], \n",
    "             test_segment_ids[:1]])\n",
    "r, r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Model\", \"config\": {\"name\": \"model\", \"layers\": [{\"name\": \"input_ids\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 512], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"input_ids\"}, \"inbound_nodes\": []}, {\"name\": \"input_masks\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 512], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"input_masks\"}, \"inbound_nodes\": []}, {\"name\": \"segment_ids\", \"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 512], \"dtype\": \"float32\", \"sparse\": false, \"name\": \"segment_ids\"}, \"inbound_nodes\": []}, {\"name\": \"bert_layer_1\", \"class_name\": \"BertLayer\", \"config\": {\"name\": \"bert_layer_1\", \"trainable\": true, \"dtype\": \"float32\"}, \"inbound_nodes\": [[[\"input_ids\", 0, 0, {}], [\"input_masks\", 0, 0, {}], [\"segment_ids\", 0, 0, {}]]]}, {\"name\": \"dense\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 256, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null, \"dtype\": \"float32\"}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {\"dtype\": \"float32\"}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"bert_layer_1\", 0, 0, {}]]]}, {\"name\": \"dense_1\", \"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null, \"dtype\": \"float32\"}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {\"dtype\": \"float32\"}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}, \"inbound_nodes\": [[[\"dense\", 0, 0, {}]]]}], \"input_layers\": [[\"input_ids\", 0, 0], [\"input_masks\", 0, 0], [\"segment_ids\", 0, 0]], \"output_layers\": [[\"dense_1\", 0, 0]]}, \"keras_version\": \"2.1.6-tf\", \"backend\": \"tensorflow\"}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_arch_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
