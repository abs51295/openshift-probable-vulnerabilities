{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1881 entries, 0 to 1880\n",
      "Data columns (total 12 columns):\n",
      "CVE ID               1328 non-null object\n",
      "Package name         1804 non-null object\n",
      "Ecosystem            1874 non-null object\n",
      "GH issue             715 non-null object\n",
      "GH PR                373 non-null object\n",
      "GH Commit            948 non-null object\n",
      "Bugzilla             259 non-null object\n",
      "ML                   482 non-null object\n",
      "Other sources        65 non-null object\n",
      "Issue Reported on    51 non-null object\n",
      "CVE reported on      58 non-null object\n",
      "Fixed on             76 non-null object\n",
      "dtypes: object(12)\n",
      "memory usage: 176.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cve_df = pd.read_csv('./data/go_cves_positive_links-may2019.csv')\n",
    "cve_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maven     1272\n",
       "python     259\n",
       "ruby       182\n",
       "go         153\n",
       "nuget        6\n",
       "npm          1\n",
       "php          1\n",
       "Name: Ecosystem, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cve_df.Ecosystem.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues Before: (715,)\n",
      "Issues After: (759,)\n",
      "PRs Before: (373,)\n",
      "PRs After: (415,)\n",
      "Commits Before: (948,)\n",
      "Commits After: (1153,)\n"
     ]
    }
   ],
   "source": [
    "gh_issues = cve_df['GH issue'].dropna().values\n",
    "print('Issues Before:', gh_issues.shape)\n",
    "gh_issues = [item.strip().split('\\n') for item in gh_issues]\n",
    "gh_issues = np.array([str(item.strip()) for sublist in gh_issues for item in sublist])\n",
    "print('Issues After:', gh_issues.shape)\n",
    "\n",
    "gh_prs = cve_df['GH PR'].dropna().values\n",
    "print('PRs Before:', gh_prs.shape)\n",
    "gh_prs = [item.strip().split('\\n') for item in gh_prs]\n",
    "gh_prs = np.array([str(item.strip()) for sublist in gh_prs for item in sublist])\n",
    "print('PRs After:', gh_prs.shape)\n",
    "\n",
    "gh_commits = cve_df['GH Commit'].dropna().values\n",
    "print('Commits Before:', gh_commits.shape)\n",
    "gh_commits = [item.strip().split('\\n') for item in gh_commits]\n",
    "gh_commits = np.array([str(item.strip()) for sublist in gh_commits for item in sublist])\n",
    "print('Commits After:', gh_commits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2327,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_links = np.concatenate((gh_issues, gh_commits, gh_prs))\n",
    "gh_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2218,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_links = np.unique(gh_links)\n",
    "gh_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_cve_links(links):\n",
    "    \n",
    "    issues = []\n",
    "    prs = []\n",
    "    commits = []\n",
    "    \n",
    "    gh_pr_commit_pattern = r'(.*github.com/.*?)/pull/(.*?)/commits/(.*)'\n",
    "    gh_issue_pattern = r'(.*github.com/.*?)/issues/(.*)'    \n",
    "    gh_pr_pattern = r'(.*github.com/.*?)/pull/([0-9]*)'\n",
    "    gh_commit_pattern = r'(.*github.com/.*?)/commit/(.*)'\n",
    "    gh_compare_pattern = r'(.*github.com/.*?)/compare/.*'\n",
    "    \n",
    "    for link in tqdm(links):\n",
    "        \n",
    "        if re.search(gh_pr_commit_pattern, link, re.I):\n",
    "            matches = re.search(gh_pr_commit_pattern, link, re.I).groups()\n",
    "            pr = matches[0]+'/pull/'+matches[1].rstrip('/')\n",
    "            prs.append(pr)\n",
    "            \n",
    "            cm = matches[0]+'/commit/'+matches[2].rstrip('/')\n",
    "            commits.append(cm)\n",
    "            \n",
    "        elif re.search(gh_issue_pattern, link, re.I):\n",
    "            issues.append(link.rstrip('/'))   \n",
    "            \n",
    "        elif re.search(gh_pr_pattern, link, re.I):\n",
    "            matches = re.search(gh_pr_pattern, link, re.I).groups()\n",
    "            repo_name = matches[0]\n",
    "            pr = repo_name+'/pull/'+matches[1].rstrip('/')\n",
    "            prs.append(pr)\n",
    "            \n",
    "            patch_link = pr+'.patch'\n",
    "            response = requests.get(patch_link)\n",
    "            if response.status_code != 200:\n",
    "                print('Failed for link:'+patch_link)\n",
    "                # log failure here not print\n",
    "            else:\n",
    "                data = response.text\n",
    "                commit_hashes = re.findall(r'(?:\\n|^)from (.*?)\\s', data, re.I)\n",
    "                commit_hashes = [item for item in commit_hashes if item.isalnum()]\n",
    "                commit_links = [repo_name+'/commit/'+item.rstrip('/') for item in commit_hashes]\n",
    "                commits.extend(commit_links)                  \n",
    "            \n",
    "        elif re.search(gh_commit_pattern, link, re.I):\n",
    "            link = link.split('#')[0].strip().rstrip('/')\n",
    "            commits.append(link)\n",
    "            \n",
    "        elif re.search(gh_compare_pattern, link, re.I):\n",
    "            match = re.search(gh_compare_pattern, link, re.I).groups()\n",
    "            repo_name = match[0]\n",
    "            patch_link = link.rstrip('/')+'.patch'\n",
    "            response = requests.get(patch_link)\n",
    "            if response.status_code != 200:\n",
    "                print('Failed for link:'+patch_link)\n",
    "                # log failure here not print\n",
    "            else:\n",
    "                data = response.text\n",
    "                commit_hashes = re.findall(r'(?:\\n|^)from (.*?)\\s', data, re.I)\n",
    "                commit_hashes = [item for item in commit_hashes if item.isalnum()]\n",
    "                commit_links = [repo_name+'/commit/'+item.rstrip('/') for item in commit_hashes]\n",
    "                commits.extend(commit_links)\n",
    "                \n",
    "    \n",
    "    issues = np.array(list(set(issues)))\n",
    "    prs = np.array(list(set(prs)))\n",
    "    commits = np.array(list(set(commits)))\n",
    "            \n",
    "    return issues, prs, commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 523/2218 [00:58<04:02,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for link:https://github.com/apache/tomcat70/compare/6b41fb05c0f6af5e6cc103ac8e5ae9da5f128606...e519f4e86bf3447934f1c399ecaff8a222e38241.patch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 528/2218 [00:58<03:11,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for link:https://github.com/apache/tomcat70/compare/72a8692370b4323f4d05b166f48a0913801fbe4f...a27df4fd31b1cd85f100b8b94e3b33dde92a3c0a.patch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2218/2218 [03:30<00:00, 10.55it/s]\n"
     ]
    }
   ],
   "source": [
    "issues, prs, commits = get_github_cve_links(gh_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((286,), (382,), (3201,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues.shape, prs.shape, commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_df = pd.DataFrame(issues, columns=['issue'])\n",
    "pr_df = pd.DataFrame(prs, columns=['pull_request'])\n",
    "commits_df = pd.DataFrame(commits, columns=['commit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_df.to_csv('./data/gh_cve_issue_links.csv', index=False)\n",
    "pr_df.to_csv('./data/gh_cve_pr_links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_df = cve_df[cve_df.Ecosystem == 'go']\n",
    "go_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues Before: (36,)\n",
      "Issues After: (39,)\n",
      "PRs Before: (40,)\n",
      "PRs After: (45,)\n",
      "Commits Before: (73,)\n",
      "Commits After: (78,)\n"
     ]
    }
   ],
   "source": [
    "gh_issues = go_df['GH issue'].dropna().values\n",
    "print('Issues Before:', gh_issues.shape)\n",
    "gh_issues = [item.strip().split('\\n') for item in gh_issues]\n",
    "gh_issues = np.array([str(item.strip()) for sublist in gh_issues for item in sublist])\n",
    "print('Issues After:', gh_issues.shape)\n",
    "\n",
    "gh_prs = go_df['GH PR'].dropna().values\n",
    "print('PRs Before:', gh_prs.shape)\n",
    "gh_prs = [item.strip().split('\\n') for item in gh_prs]\n",
    "gh_prs = np.array([str(item.strip()) for sublist in gh_prs for item in sublist])\n",
    "print('PRs After:', gh_prs.shape)\n",
    "\n",
    "gh_commits = go_df['GH Commit'].dropna().values\n",
    "print('Commits Before:', gh_commits.shape)\n",
    "gh_commits = [item.strip().split('\\n') for item in gh_commits]\n",
    "gh_commits = np.array([str(item.strip()) for sublist in gh_commits for item in sublist])\n",
    "print('Commits After:', gh_commits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_links = np.concatenate((gh_issues, gh_commits, gh_prs))\n",
    "gh_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_links = np.unique(gh_links)\n",
    "gh_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_links = np.array([item for item in gh_links if 'github' in item])\n",
    "gh_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://github.com/apache/thrift/commit/2007783e874d524a46b818598a45078448ecc53e',\n",
       "       'https://github.com/apache/thrift/pull/1061',\n",
       "       'https://github.com/astaxie/beego/commit/9865779f149669777ee33aae71cd29c8db8ffd66',\n",
       "       'https://github.com/astaxie/beego/pull/3383',\n",
       "       'https://github.com/brancz/kube-rbac-proxy/commit/c41c4dee92abc0859d952559e37cf9ad8a442789',\n",
       "       'https://github.com/brancz/kube-rbac-proxy/pull/27',\n",
       "       'https://github.com/cisco/node-jose/pull/88',\n",
       "       'https://github.com/cloudflare/cfssl/commit/f74c74db7f22df0051d7f872b5161dfa2a797ace',\n",
       "       'https://github.com/cloudflare/cfssl/pull/776',\n",
       "       'https://github.com/cloudfoundry-incubator/bits-service/commit/9e4010e42a4b462fef69889a453b5c32d56e3100'],\n",
       "      dtype='<U130')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'.*?github.com/(.*?/.*?)/', re.I)\n",
    "repo_names = np.array(list(filter(None,[pattern.search(item).group(1) \n",
    "                                            if pattern.search(item) else None \n",
    "                                               for item in gh_links])))\n",
    "repo_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_names = np.unique(repo_names)\n",
    "repo_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/95/64e92560983db41ff1de7c08839f38ae7c5326a8aad71f5e893098cd1c85/google_cloud_bigquery-1.15.0-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 3.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.0 (from google-cloud-bigquery)\n",
      "  Downloading https://files.pythonhosted.org/packages/98/7f/ff56aec313787577e262d5a2e306c04aef61c5c274699ff9fb40095e6691/google_cloud_core-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from google-cloud-bigquery) (3.8.0)\n",
      "Collecting google-resumable-media>=0.3.1 (from google-cloud-bigquery)\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/5d/4bc5c28c252a62efe69ed1a1561da92bd5af8eca0cdcdf8e60354fae9b29/google_resumable_media-0.3.2-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2.0.0dev,>=1.11.0 (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/78/bbd685dda48a291b4cc81568ed3e1a89af7a61958dc88a3d52a819b1919d/google_api_core-1.13.0-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 28.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.0->google-cloud-bigquery) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.0->google-cloud-bigquery) (41.0.1)\n",
      "Collecting googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 (from google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/ee/e59e74ecac678a14d6abefb9054f0bbcb318a6452a30df3776f133886d7d/googleapis-common-protos-1.6.0.tar.gz\n",
      "Requirement already satisfied: pytz in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery) (2019.1)\n",
      "Collecting google-auth<2.0dev,>=0.4.0 (from google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/9b/ed0516cc1f7609fb0217e3057ff4f0f9f3e3ce79a369c6af4a6c5ca25664/google_auth-1.6.3-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 35.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery) (2.22.0)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/f0/b03e00ce9fddf4827c42df1c3ce10c74eadebfb706231e8d6d1c356a4062/pyasn1_modules-0.2.5-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 42.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools>=2.0.0 (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting rsa>=3.1.4 (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/redanalyze/anaconda3/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery) (3.0.4)\n",
      "Collecting pyasn1<0.5.0,>=0.4.1 (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.11.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-bigquery)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/7c/c9386b82a25115cccf1903441bba3cbadcfae7b678a20167347fa8ded34c/pyasn1-0.4.5-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 45.1MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: googleapis-common-protos\n",
      "  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/redanalyze/.cache/pip/wheels/9e/3d/a2/1bec8bb7db80ab3216dbc33092bb7ccd0debfb8ba42b5668d5\n",
      "Successfully built googleapis-common-protos\n",
      "Installing collected packages: googleapis-common-protos, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery\n",
      "Successfully installed cachetools-3.1.1 google-api-core-1.13.0 google-auth-1.6.3 google-cloud-bigquery-1.15.0 google-cloud-core-1.0.2 google-resumable-media-0.3.2 googleapis-common-protos-1.6.0 pyasn1-0.4.5 pyasn1-modules-0.2.5 rsa-4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bq_utils as bqu\n",
    "import json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bq_utils.BigQueryHelper at 0x7fdfa2d3d080>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'bq_key.json'\n",
    "gh_archive = bqu.BigQueryHelper(active_project= \"githubarchive\", \n",
    "                                dataset_name = \"year\")\n",
    "gh_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_archive.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.9256986239925"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT  type, count(*)\n",
    "        FROM `githubarchive.year.20*`\n",
    "        WHERE _TABLE_SUFFIX IN ('16', '17', '18')\n",
    "        AND repo.name in {repos}\n",
    "        GROUP BY type\n",
    "\"\"\".format(repos=tuple(repo_names))\n",
    "gh_archive.estimate_query_size(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PullRequestEvent</td>\n",
       "      <td>168759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PullRequestReviewCommentEvent</td>\n",
       "      <td>236683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PushEvent</td>\n",
       "      <td>117640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IssueCommentEvent</td>\n",
       "      <td>1623715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>IssuesEvent</td>\n",
       "      <td>195866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             type      f0_\n",
       "0                PullRequestEvent   168759\n",
       "2   PullRequestReviewCommentEvent   236683\n",
       "6                       PushEvent   117640\n",
       "8               IssueCommentEvent  1623715\n",
       "13                    IssuesEvent   195866"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = gh_archive.query_to_pandas(query)\n",
    "df[df.type.isin(['PushEvent', 'PullRequestEvent', 'IssuesEvent', \n",
    "                'PullRequestReviewCommentEvent', 'IssueCommentEvent'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583.8375390227884"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    repo.name as repo_name, \n",
    "    type as event_type, \n",
    "    actor.id as actor_id,\n",
    "    actor.login as actor_name,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.action') as issue_status,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.url') as issue_api_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.html_url') as issue_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.user.login') as issue_creator_name,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.user.url') as issue_creator_api_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.user.html_url') as issue_creator_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.comments') as comment_count,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.id') as issue_id,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.number') as issue_number,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.created_at') as issue_created_at,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.updated_at') as issue_updated_at,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.issue.closed_at') as issue_closed_at,\n",
    "    TRIM(REGEXP_REPLACE(\n",
    "             REGEXP_REPLACE(\n",
    "                 JSON_EXTRACT_SCALAR(payload, '$.issue.title'), \n",
    "                 r'\\\\r\\\\n|\\\\r|\\\\n', \n",
    "                 ' '),\n",
    "             r'\\s{2,}', \n",
    "             ' ')) as issue_title,\n",
    "    TRIM(REGEXP_REPLACE(\n",
    "             REGEXP_REPLACE(\n",
    "                 JSON_EXTRACT_SCALAR(payload, '$.issue.body'), \n",
    "                 r'\\\\r\\\\n|\\\\r|\\\\n', \n",
    "                 ' '),\n",
    "             r'\\s{2,}', \n",
    "             ' ')) as issue_body\n",
    "        \n",
    "FROM `githubarchive.year.20*`\n",
    "    WHERE _TABLE_SUFFIX IN ('16', '17', '18')\n",
    "    AND repo.name in {repos}\n",
    "    AND type = 'IssuesEvent'\n",
    "    \"\"\".replace('{repos}', str(tuple(repo_names)))\n",
    "gh_archive.estimate_query_size(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_df = gh_archive.query_to_pandas(query)\n",
    "issues_df.to_csv('./data/GH_unlabeled_issues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583.8375390227884"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    repo.name as repo_name, \n",
    "    type as event_type, \n",
    "    actor.id as actor_id,\n",
    "    actor.login as actor_name,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.action') as pr_status,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.id') as pr_id,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.number') as pr_number,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.url') as pr_api_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.html_url') as pr_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.diff_url') as pr_diff_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.patch_url') as pr_patch_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.user.login') as pr_creator_name,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.user.url') as pr_creator_api_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.user.html_url') as pr_creator_url,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.created_at') as pr_created_at,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.updated_at') as pr_updated_at,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.closed_at') as pr_closed_at,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.merged_at') as pr_merged_at,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.merged') as pr_merged_status,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.comments') as pr_comments_count,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.review_comments') as pr_review_comments_count,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.commits') as pr_commits_count,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.additions') as pr_additions_count,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.deletions') as pr_deletions_count,\n",
    "    JSON_EXTRACT_SCALAR(payload, '$.pull_request.changed_files') as pr_changed_files_count,    \n",
    "    TRIM(REGEXP_REPLACE(\n",
    "             REGEXP_REPLACE(\n",
    "                 JSON_EXTRACT_SCALAR(payload, '$.pull_request.title'), \n",
    "                 r'\\\\r\\\\n|\\\\r|\\\\n', \n",
    "                 ' '),\n",
    "             r'\\s{2,}', \n",
    "             ' ')) as pr_title,\n",
    "    TRIM(REGEXP_REPLACE(\n",
    "             REGEXP_REPLACE(\n",
    "                 JSON_EXTRACT_SCALAR(payload, '$.pull_request.body'), \n",
    "                 r'\\\\r\\\\n|\\\\r|\\\\n', \n",
    "                 ' '),\n",
    "             r'\\s{2,}', \n",
    "             ' ')) as pr_body\n",
    "        \n",
    "FROM `githubarchive.year.20*`\n",
    "    WHERE _TABLE_SUFFIX IN ('16', '17', '18')\n",
    "    AND repo.name in {repos}\n",
    "    AND type = 'PullRequestEvent'\n",
    "\"\"\".replace('{repos}', str(tuple(repo_names)))\n",
    "gh_archive.estimate_query_size(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df = gh_archive.query_to_pandas(query)\n",
    "prs_df.to_csv('./data/GH_unlabeled_prs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = pd.DataFrame(repo_names, columns=['repo_names'])\n",
    "repos.to_csv('./data/Go_GH_repo_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 195866 entries, 0 to 195865\n",
      "Data columns (total 18 columns):\n",
      "repo_name                195866 non-null object\n",
      "event_type               195866 non-null object\n",
      "actor_id                 195866 non-null int64\n",
      "actor_name               195866 non-null object\n",
      "issue_status             195866 non-null object\n",
      "issue_api_url            195866 non-null object\n",
      "issue_url                195866 non-null object\n",
      "issue_creator_name       195866 non-null object\n",
      "issue_creator_api_url    195866 non-null object\n",
      "issue_creator_url        195866 non-null object\n",
      "comment_count            195866 non-null int64\n",
      "issue_id                 195866 non-null int64\n",
      "issue_number             195866 non-null int64\n",
      "issue_created_at         195866 non-null object\n",
      "issue_updated_at         195866 non-null object\n",
      "issue_closed_at          89574 non-null object\n",
      "issue_title              195863 non-null object\n",
      "issue_body               194279 non-null object\n",
      "dtypes: int64(4), object(14)\n",
      "memory usage: 26.9+ MB\n"
     ]
    }
   ],
   "source": [
    "gh_bq_issues = pd.read_csv('./data/GH_unlabeled_issues.csv')\n",
    "gh_bq_issues.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86356, 18)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_issues = gh_bq_issues[~pd.isnull(gh_bq_issues.issue_id)]\n",
    "gh_bq_issues.issue_created_at = pd.to_datetime(gh_bq_issues.issue_created_at)\n",
    "gh_bq_issues.issue_updated_at = pd.to_datetime(gh_bq_issues.issue_updated_at)\n",
    "gh_bq_issues.issue_closed_at = pd.to_datetime(gh_bq_issues.issue_closed_at)\n",
    "\n",
    "gh_bq_issues = gh_bq_issues[gh_bq_issues.issue_created_at.dt.year <= 2019]\n",
    "gh_bq_issues = gh_bq_issues[gh_bq_issues.issue_updated_at.dt.year <= 2019]\n",
    "gh_bq_issues = gh_bq_issues[gh_bq_issues.issue_closed_at.dt.year <= 2019]\n",
    "\n",
    "gh_bq_issues = gh_bq_issues.loc[gh_bq_issues.groupby('issue_id').issue_updated_at.idxmax(skipna=False)]\n",
    "\n",
    "gh_bq_issues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CVE issues: 289\n"
     ]
    }
   ],
   "source": [
    "gh_cve_issue_links = pd.read_csv('./data/gh_cve_issue_links.csv')\n",
    "cve_issue_links = gh_cve_issue_links.issue.tolist()\n",
    "cve_issue_links.extend(['https://github.com/golang/go/issues/30642', \n",
    "                        'https://github.com/golang/go/issues/30794', \n",
    "                        'https://github.com/hashicorp/consul/issues/5423'])\n",
    "print('Total CVE issues:', len(cve_issue_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 18)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cve_issues = gh_bq_issues[gh_bq_issues.issue_url.isin(cve_issue_links)]\n",
    "cve_issues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues not found in unlabeled data:  267\n"
     ]
    }
   ],
   "source": [
    "found_issue_urls = cve_issues.issue_url.tolist()\n",
    "not_found_issue_urls = list(set(cve_issue_links) - set(found_issue_urls))\n",
    "print('Issues not found in unlabeled data: ', len(not_found_issue_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 267/267 [00:57<00:00,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found missing issues: 267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "not_found_api_urls = ['https://api.github.com/repos/'+re.search(r'.*github.com/(.*)', link, re.I).groups()[0]\n",
    "                          for link in not_found_issue_urls]\n",
    "\n",
    "data = []\n",
    "for link in tqdm(not_found_api_urls):\n",
    "    response = requests.get(link,\n",
    "                            auth=('dipanjanS', ''))\n",
    "    if not response.status_code == 200:\n",
    "        print('Failed for link: '+link)\n",
    "        # log this later\n",
    "    else:\n",
    "        content = response.json()\n",
    "        issue_dict = {\n",
    "            'repo_name': re.search(r'.*github.com/repos/(.*?)/issues', \n",
    "                                   link, re.I).groups()[0],\n",
    "            'event_type': 'IssuesEvent',\n",
    "            'actor_id': content.get('user').get('id'),\n",
    "            'actor_name': content.get('user').get('login'),\n",
    "            'issue_status': content.get('state'),\n",
    "            'issue_api_url': content.get('url'),\n",
    "            'issue_url': content.get('html_url'),\n",
    "            'issue_creator_name': content.get('user').get('login'),\n",
    "            'issue_creator_api_url': content.get('user').get('url'),\n",
    "            'issue_creator_url': content.get('user').get('html_url'),\n",
    "            'comment_count': content.get('comments'),\n",
    "            'issue_id': content.get('id'),\n",
    "            'issue_number': content.get('number'),\n",
    "            'issue_created_at': content.get('created_at'),\n",
    "            'issue_updated_at': content.get('updated_at'),\n",
    "            'issue_closed_at': content.get('closed_at'),\n",
    "            'issue_title': content.get('title'),\n",
    "            'issue_body': content.get('body')\n",
    "        }\n",
    "        if issue_dict:\n",
    "            data.append(issue_dict)\n",
    "\n",
    "print('Found missing issues:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289, 18), (86334, 18))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cve_issues_nf = pd.DataFrame(data)\n",
    "cve_issues_nf = cve_issues_nf[cve_issues.columns.tolist()]\n",
    "\n",
    "gh_bq_issues_cve = pd.concat([cve_issues, cve_issues_nf], axis=0).reset_index(drop=True)\n",
    "gh_bq_issues_negative = gh_bq_issues.drop(cve_issues.index.tolist()).reset_index(drop=True)\n",
    "gh_bq_issues_cve.shape, gh_bq_issues_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86623, 19)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_issues_cve['class_label'] = 2\n",
    "gh_bq_issues_negative['class_label'] = 0\n",
    "\n",
    "gh_bq_issues_processed = pd.concat([gh_bq_issues_negative, gh_bq_issues_cve], axis=0).reset_index(drop=True)\n",
    "gh_bq_issues_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_bq_issues_processed.to_csv('./data/GH_cve_labeled_issues.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168759 entries, 0 to 168758\n",
      "Data columns (total 27 columns):\n",
      "repo_name                   168759 non-null object\n",
      "event_type                  168759 non-null object\n",
      "actor_id                    168759 non-null int64\n",
      "actor_name                  168759 non-null object\n",
      "pr_status                   168759 non-null object\n",
      "pr_id                       168759 non-null int64\n",
      "pr_number                   168759 non-null int64\n",
      "pr_api_url                  168759 non-null object\n",
      "pr_url                      168759 non-null object\n",
      "pr_diff_url                 168759 non-null object\n",
      "pr_patch_url                168759 non-null object\n",
      "pr_creator_name             168759 non-null object\n",
      "pr_creator_api_url          168759 non-null object\n",
      "pr_creator_url              168759 non-null object\n",
      "pr_created_at               168759 non-null object\n",
      "pr_updated_at               168759 non-null object\n",
      "pr_closed_at                83684 non-null object\n",
      "pr_merged_at                65214 non-null object\n",
      "pr_merged_status            168759 non-null bool\n",
      "pr_comments_count           168759 non-null int64\n",
      "pr_review_comments_count    168759 non-null int64\n",
      "pr_commits_count            168759 non-null int64\n",
      "pr_additions_count          168759 non-null int64\n",
      "pr_deletions_count          168759 non-null int64\n",
      "pr_changed_files_count      168759 non-null int64\n",
      "pr_title                    168759 non-null object\n",
      "pr_body                     153862 non-null object\n",
      "dtypes: bool(1), int64(9), object(17)\n",
      "memory usage: 33.6+ MB\n"
     ]
    }
   ],
   "source": [
    "gh_bq_prs = pd.read_csv('./data/GH_unlabeled_prs.csv')\n",
    "gh_bq_prs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_bq_prs = gh_bq_prs[~pd.isnull(gh_bq_prs.pr_title)]\n",
    "\n",
    "def fill_missing_links(record):\n",
    "    repo_name = record['repo_name']\n",
    "    pr_number = str(record['pr_number'])\n",
    "    if pd.isnull(record['pr_api_url']):\n",
    "        record['pr_api_url'] = 'https://api.github.com/repos/'+repo_name+'/pulls/'+pr_number\n",
    "    if pd.isnull(record['pr_url']):\n",
    "        record['pr_url'] = 'https://github.com/'+repo_name+'/pull/'+pr_number \n",
    "    if pd.isnull(record['pr_diff_url']):\n",
    "        record['pr_diff_url'] = 'https://github.com/'+repo_name+'/pull/'+pr_number+'.diff' \n",
    "    if pd.isnull(record['pr_patch_url']):\n",
    "        record['pr_patch_url'] = 'https://github.com/'+repo_name+'/pull/'+pr_number+'.patch'\n",
    "    return record\n",
    "\n",
    "gh_bq_prs = gh_bq_prs.apply(lambda row: fill_missing_links(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 27), (168759, 27))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_prs_missing_info = gh_bq_prs[pd.isnull(gh_bq_prs.pr_updated_at)]\n",
    "gh_bq_prs_full_info = gh_bq_prs[~pd.isnull(gh_bq_prs.pr_updated_at)]\n",
    "gh_bq_prs_missing_info.shape, gh_bq_prs_full_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 27)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_prs_missing_info = gh_bq_prs_missing_info.drop_duplicates(subset=['pr_url'], keep=\"last\")\n",
    "gh_bq_prs_missing_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65176, 27)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_prs_full_info.pr_created_at = pd.to_datetime(gh_bq_prs_full_info.pr_created_at)\n",
    "gh_bq_prs_full_info.pr_updated_at = pd.to_datetime(gh_bq_prs_full_info.pr_updated_at)\n",
    "gh_bq_prs_full_info.pr_closed_at = pd.to_datetime(gh_bq_prs_full_info.pr_closed_at)\n",
    "gh_bq_prs_full_info.pr_merged_at = pd.to_datetime(gh_bq_prs_full_info.pr_merged_at)\n",
    "\n",
    "gh_bq_prs_full_info = gh_bq_prs_full_info[gh_bq_prs_full_info.pr_created_at.dt.year <= 2019]\n",
    "gh_bq_prs_full_info = gh_bq_prs_full_info[gh_bq_prs_full_info.pr_updated_at.dt.year <= 2019]\n",
    "gh_bq_prs_full_info = gh_bq_prs_full_info[gh_bq_prs_full_info.pr_closed_at.dt.year <= 2019]\n",
    "gh_bq_prs_full_info = gh_bq_prs_full_info[gh_bq_prs_full_info.pr_merged_at.dt.year <= 2019]\n",
    "\n",
    "gh_bq_prs_full_info = gh_bq_prs_full_info.loc[gh_bq_prs_full_info.groupby('pr_url').pr_updated_at.idxmax(skipna=False)]\n",
    "gh_bq_prs_full_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65176, 27)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_prs = pd.concat([gh_bq_prs_full_info, gh_bq_prs_missing_info], axis=0).reset_index(drop=True)\n",
    "gh_bq_prs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CVE PRs: 382\n"
     ]
    }
   ],
   "source": [
    "gh_cve_pr_links = pd.read_csv('./data/gh_cve_pr_links.csv')\n",
    "cve_pr_links = gh_cve_pr_links.pull_request.tolist()\n",
    "print('Total CVE PRs:', len(cve_pr_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 27)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cve_prs = gh_bq_prs[gh_bq_prs.pr_url.isin(cve_pr_links)]\n",
    "cve_prs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRs not found in unlabeled data: 352\n"
     ]
    }
   ],
   "source": [
    "found_pr_urls = cve_prs.pr_url.tolist()\n",
    "not_found_pr_urls = list(set(cve_pr_links) - set(found_pr_urls))\n",
    "print('PRs not found in unlabeled data:', len(not_found_pr_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352/352 [02:21<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "not_found_api_urls = [re.sub('/pull/', '/pulls/', \n",
    "                             ('https://api.github.com/repos/'+re.search(r'.*github.com/(.*)', link, re.I)\n",
    "                              .groups()[0]), re.I)\n",
    "                          for link in not_found_pr_urls]\n",
    "\n",
    "data = []\n",
    "for link in tqdm(not_found_api_urls):\n",
    "    response = requests.get(link,\n",
    "                            auth=('dipanjanS', ''))\n",
    "    if not response.status_code == 200:\n",
    "        print('Failed for link: '+link)\n",
    "        # log this later\n",
    "    else:\n",
    "        content = response.json()\n",
    "        pr_dict = {\n",
    "            'repo_name': re.search(r'.*github.com/repos/(.*?)/pulls', \n",
    "                                   link, re.I).groups()[0],\n",
    "            'event_type': 'PullRequestEvent',\n",
    "            'actor_id': content.get('user').get('id'),\n",
    "            'actor_name': content.get('user').get('login'),\n",
    "            'pr_status': content.get('state'),\n",
    "            'pr_id': content.get('id'),\n",
    "            'pr_number': content.get('number'),\n",
    "            'pr_api_url': content.get('url'),\n",
    "            'pr_url': content.get('html_url'),\n",
    "            'pr_diff_url': content.get('diff_url'),\n",
    "            'pr_patch_url': content.get('patch_url'),\n",
    "            'pr_creator_name': content.get('user').get('login'),\n",
    "            'pr_creator_api_url': content.get('user').get('url'),\n",
    "            'pr_creator_url': content.get('user').get('html_url'),         \n",
    "            'pr_created_at': content.get('created_at'),\n",
    "            'pr_updated_at': content.get('updated_at'),\n",
    "            'pr_closed_at': content.get('closed_at'),\n",
    "            'pr_merged_at': content.get('merged_at'),\n",
    "            'pr_merged_status': content.get('merged'),\n",
    "            'pr_comments_count': content.get('comments'),\n",
    "            'pr_review_comments_count': content.get('review_comments'),\n",
    "            'pr_commits_count': content.get('commits'),\n",
    "            'pr_additions_count': content.get('additions'),\n",
    "            'pr_deletions_count': content.get('deletions'),\n",
    "            'pr_changed_files_count': content.get('changed_files'),\n",
    "            'pr_title': content.get('title'),\n",
    "            'pr_body': content.get('body')\n",
    "        }\n",
    "        if pr_dict:\n",
    "            data.append(pr_dict)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((382, 27), (65146, 27))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cve_prs_nf = pd.DataFrame(data)\n",
    "cve_prs_nf = cve_prs_nf[cve_prs.columns.tolist()]\n",
    "\n",
    "gh_bq_prs_cve = pd.concat([cve_prs, cve_prs_nf], axis=0).reset_index(drop=True)\n",
    "gh_bq_prs_negative = gh_bq_prs.drop(cve_prs.index.tolist()).reset_index(drop=True)\n",
    "gh_bq_prs_cve.shape, gh_bq_prs_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65528, 28)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gh_bq_prs_cve['class_label'] = 2\n",
    "gh_bq_prs_negative['class_label'] = 0\n",
    "\n",
    "gh_bq_prs_processed = pd.concat([gh_bq_prs_negative, gh_bq_prs_cve], axis=0).reset_index(drop=True)\n",
    "gh_bq_prs_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_bq_prs_processed.to_csv('./data/GH_cve_labeled_prs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152151 entries, 0 to 152150\n",
      "Data columns (total 2 columns):\n",
      "description    152151 non-null object\n",
      "class_label    152151 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "issues_df = pd.read_csv('./data/GH_cve_labeled_issues.csv')\n",
    "prs_df = pd.read_csv('./data/GH_cve_labeled_prs.csv')\n",
    "\n",
    "issues_df['issue_title'] = issues_df.issue_title.fillna('')\n",
    "issues_df['issue_body'] = issues_df.issue_body.fillna('')\n",
    "issues_df['description'] = issues_df['issue_title'].map(str) + ' ' + issues_df['issue_body']\n",
    "\n",
    "prs_df['pr_title'] = prs_df.pr_title.fillna('')\n",
    "prs_df['pr_body'] = prs_df.pr_body.fillna('')\n",
    "prs_df['description'] = prs_df['pr_title'].map(str) + ' ' + prs_df['pr_body']\n",
    "\n",
    "df = pd.concat([issues_df[['description', 'class_label']], \n",
    "                prs_df[['description', 'class_label']]], axis=0).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    151480\n",
       "2       671\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.class_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def security_vulnerability_labeler(issue_description):\n",
    "    strong_vuln_pattern = ('(?i)(advisory|attack|(un)?authoriz(e|ation)|'\n",
    "                           'clickjack|cross.site|csrf|\\\\bCVE.*?\\\\b|deadlock|'\n",
    "                           'denial.of.service|\\\\bEOP\\\\b|exploit|hijack|'\n",
    "                           'infinite.loop|malicious|\\\\bNVD\\\\b|OSVDB|'\n",
    "                           '\\\\bRCE\\\\b|\\\\bReDoS\\\\b|\\\\bDDoS\\\\b|remote.code.execution|'\n",
    "                           'security|victim|\\\\bvuln|\\\\bXEE\\\\b|\\\\bXSRF\\\\b|'\n",
    "                           '\\\\bXSS\\\\b|\\\\bXXE\\\\b)')\n",
    "\n",
    "    medium_vuln_pattern = ('(?i)(authenticat(e|ion)|brute force|bypass|'\n",
    "                           'constant.time|crack|credential|\\\\bDoS\\\\b|'\n",
    "                           'expos(e|ing)|hack|harden|injection|lockout|'\n",
    "                           'overflow|password|\\\\bPoC\\\\b|proof.of.concept|'\n",
    "                           'poison|priv(ilege|elege|elage|lage)|\\\\b(in)?secur(e|ity)|'\n",
    "                           '(de)?serializ|spoof|timing|traversal)')\n",
    "\n",
    "    low_vuln_pattern = ('(?i)(abuse|compliant|constant.time|credential|\\\\bcrypto|'\n",
    "                        'escalate|exhaustion|forced|infinite|RFC\\\\d{4,5})')\n",
    "    \n",
    "    if (re.findall(low_vuln_pattern, issue_description, re.I) or\n",
    "        re.findall(medium_vuln_pattern, issue_description, re.I) or\n",
    "        re.findall(strong_vuln_pattern, issue_description, re.I)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df = df[df.class_label == 0]\n",
    "positive_df = df[df.class_label == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "negative_df['label'] = negative_df.apply(lambda row: security_vulnerability_labeler(row.description), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/redanalyze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "positive_df['label'] = positive_df['class_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152151 entries, 0 to 152150\n",
      "Data columns (total 2 columns):\n",
      "description    152151 non-null object\n",
      "label          152151 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([positive_df[['description', 'label']],\n",
    "                negative_df[['description', 'label']]], axis=0).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    128908\n",
       "1     22572\n",
       "2       671\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/GH_complete_labeled_issues_prs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
