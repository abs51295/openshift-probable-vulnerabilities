{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 835MB\r\n",
      "-rw-rw-r-- 1 redanalyze redanalyze   1MB Jun 21 12:53 'BERT - Raw TensorFlow Implementation - Sentiment Analysis.ipynb'\r\n",
      "-rw-rw-r-- 1 redanalyze redanalyze   1MB Jun 21 12:53  Untitled.ipynb\r\n",
      "-rw-rw-r-- 1 redanalyze redanalyze   1MB Jun 21 11:18  checkpoint\r\n",
      "drwxr-xr-x 2 redanalyze redanalyze   1MB Jun 21 10:02  eval\r\n",
      "-rw-rw-r-- 1 redanalyze redanalyze 623MB Jun 21 11:18  events.out.tfevents.1561110216.better-eve-instance\r\n",
      "-rw-rw-r-- 1 redanalyze redanalyze 193MB Jun 21 11:01  graph.pbtxt\r\n",
      "-rw-rw-r-- 1 redanalyze redanalyze  20MB Jun 21 03:12  imdb_movie_reviews.csv.bz2\r\n",
      "drwxrwxr-x 3 redanalyze redanalyze   1MB Jun 21 12:55  tf_models\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l --block-size=MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0621 12:58:05.233371 140059782334272 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./imdb_movie_reviews.csv.bz2', compression='bz2')\n",
    "dataset['sentiment'] = [1 if sentiment == 'positive' else 0 \n",
    "                            for sentiment in dataset['sentiment'].values]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 2), (5000, 2), (15000, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_COLUMN = 'review'\n",
    "LABEL_COLUMN = 'sentiment'\n",
    "label_list = [0, 1]\n",
    "\n",
    "train_df = dataset.iloc[:30000]\n",
    "val_df = dataset.iloc[30000:35000]\n",
    "test_df = dataset.iloc[35000:]\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:00:43.408170 140059782334272 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization info: {'do_lower_case': True, 'vocab_file': b'/tmp/tfhub_modules/5a395eafef2a37bd9fc55d7f6ae676d2a134a838/assets/vocab.txt'}\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_TFHUB = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "\n",
    "def create_tokenizer_from_tfhub_module(tfhub_module):\n",
    "    \"\"\"Get the vocab file and casing info from the TF Hub module.\"\"\"\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        # we are not fine-tuning BERT layers here\n",
    "        bert_module = tf_hub.Module(tfhub_module, trainable=False)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", \n",
    "                                        as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            print('Tokenization info: {}'.format(sess.run(tokenization_info)))\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                  tokenization_info[\"do_lower_case\"]])\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_tfhub_module(tfhub_module=BERT_MODEL_TFHUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(tfhub_module, is_predicting, input_ids, input_mask, segment_ids, \n",
    "                 labels, num_labels, finetune_layers=True):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    \n",
    "    bert_module = tf_hub.Module(tfhub_module, trainable=finetune_layers)\n",
    "    \n",
    "    bert_inputs = dict(input_ids=input_ids,\n",
    "                       input_mask=input_mask,\n",
    "                       segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\",\n",
    "                               as_dict=True)\n",
    "    \n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for sentiment data.\n",
    "    output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size],\n",
    "                                     initializer=tf.glorot_uniform_initializer(seed=SEED))\n",
    "    output_bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        \n",
    "        #  get labels from probabilities\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        \n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "        \n",
    "        # If we're training / evaluating, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        \n",
    "        return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "\n",
    "def model_fn_builder(tfhub_module, num_labels, learning_rate, num_train_steps, num_warmup_steps,\n",
    "                     finetune_layers=True):\n",
    "    \"\"\"Returns `model_fn` closure for our tf Estimator.\"\"\"\n",
    "    \n",
    "    def model_fn(features, labels, mode, params):  \n",
    "        \"\"\"The `model_fn` for tf Estimator.\"\"\"\n",
    "        \n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        # TRAINING and EVALUATION\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(tfhub_module, is_predicting, \n",
    "                                                               input_ids, input_mask, segment_ids, \n",
    "                                                               label_ids, num_labels, \n",
    "                                                               finetune_layers=finetune_layers)\n",
    "            train_op = bert.optimization.create_optimizer(loss, learning_rate, \n",
    "                                                          num_train_steps, num_warmup_steps, \n",
    "                                                          use_tpu=False)\n",
    "            \n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(label_ids, predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels)\n",
    "                true_pos = tf.metrics.true_positives(label_ids, predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids, predicted_labels)\n",
    "                false_pos = tf.metrics.false_positives(label_ids, predicted_labels)\n",
    "                false_neg = tf.metrics.false_negatives(label_ids, predicted_labels)\n",
    "                \n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "            \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "            \n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(tfhub_module, is_predicting, input_ids, \n",
    "                                                         input_mask, segment_ids, \n",
    "                                                         label_ids, num_labels, finetune_layers=False)\n",
    "            \n",
    "            predictions = {\n",
    "              'probabilities': log_probs,\n",
    "              'labels': predicted_labels\n",
    "            }\n",
    "            \n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Specify the output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(tf_random_seed=SEED)\n",
    "\n",
    "model_fn = model_fn_builder(tfhub_module=BERT_MODEL_TFHUB, num_labels=len(label_list),\n",
    "                            learning_rate=LEARNING_RATE, num_train_steps=0,\n",
    "                            num_warmup_steps=0, finetune_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:03:34.078605 140059782334272 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization info: {'do_lower_case': True, 'vocab_file': b'/tmp/tfhub_modules/5a395eafef2a37bd9fc55d7f6ae676d2a134a838/assets/vocab.txt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15000, 15000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_MODEL_TFHUB = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "MAX_SEQ_LENGTH = 128\n",
    "label_list = [0, 1]\n",
    "tokenizer = create_tokenizer_from_tfhub_module(tfhub_module=BERT_MODEL_TFHUB)\n",
    "test_documents = test_df.review.tolist()\n",
    "test_labels = test_df.sentiment.tolist()\n",
    "\n",
    "len(test_documents), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_pred_input_examples(documents):\n",
    "    input_examples = [bert.run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in documents]\n",
    "    return input_examples\n",
    "\n",
    "\n",
    "def build_bert_pred_input_features(inp_examples, label_list, max_seq_length, \n",
    "                                   tokenizer):\n",
    "    input_features = (bert.run_classifier\n",
    "                          .convert_examples_to_features(inp_examples, label_list,\n",
    "                                                      max_seq_length, tokenizer))\n",
    "    return input_features\n",
    "\n",
    "\n",
    "def build_bert_pred_input_function(inp_features, max_seq_length):\n",
    "    predict_input_fn = (bert.run_classifier\n",
    "                            .input_fn_builder(features=inp_features, \n",
    "                                            seq_length=max_seq_length, \n",
    "                                            is_training=False, \n",
    "                                            drop_remainder=False))\n",
    "    return predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.333906 140059782334272 tf_logging.py:115] Writing example 0 of 15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.338539 140059782334272 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.340023 140059782334272 tf_logging.py:115] guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] just don ' t bother . i thought i would see a movie with great su ##ps ##pen ##se and action . < br / > < br / > but it grows boring and terribly predictable after the interesting start . in the middle of the film you have a little social drama and all tension is lost because it slow ##s down the speed . towards the end the it gets better but not really great . i think the director took this movie just too serious . in such a kind of a movie even if u don ' t care about the plot at least you want some nice action . i nearly do ##zed off in the middle / main part [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.341588 140059782334272 tf_logging.py:115] tokens: [CLS] just don ' t bother . i thought i would see a movie with great su ##ps ##pen ##se and action . < br / > < br / > but it grows boring and terribly predictable after the interesting start . in the middle of the film you have a little social drama and all tension is lost because it slow ##s down the speed . towards the end the it gets better but not really great . i think the director took this movie just too serious . in such a kind of a movie even if u don ' t care about the plot at least you want some nice action . i nearly do ##zed off in the middle / main part [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2074 2123 1005 1056 8572 1012 1045 2245 1045 2052 2156 1037 3185 2007 2307 10514 4523 11837 3366 1998 2895 1012 1026 7987 1013 1028 1026 7987 1013 1028 2021 2009 7502 11771 1998 16668 21425 2044 1996 5875 2707 1012 1999 1996 2690 1997 1996 2143 2017 2031 1037 2210 2591 3689 1998 2035 6980 2003 2439 2138 2009 4030 2015 2091 1996 3177 1012 2875 1996 2203 1996 2009 4152 2488 2021 2025 2428 2307 1012 1045 2228 1996 2472 2165 2023 3185 2074 2205 3809 1012 1999 2107 1037 2785 1997 1037 3185 2130 2065 1057 2123 1005 1056 2729 2055 1996 5436 2012 2560 2017 2215 2070 3835 2895 1012 1045 3053 2079 5422 2125 1999 1996 2690 1013 2364 2112 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.342924 140059782334272 tf_logging.py:115] input_ids: 101 2074 2123 1005 1056 8572 1012 1045 2245 1045 2052 2156 1037 3185 2007 2307 10514 4523 11837 3366 1998 2895 1012 1026 7987 1013 1028 1026 7987 1013 1028 2021 2009 7502 11771 1998 16668 21425 2044 1996 5875 2707 1012 1999 1996 2690 1997 1996 2143 2017 2031 1037 2210 2591 3689 1998 2035 6980 2003 2439 2138 2009 4030 2015 2091 1996 3177 1012 2875 1996 2203 1996 2009 4152 2488 2021 2025 2428 2307 1012 1045 2228 1996 2472 2165 2023 3185 2074 2205 3809 1012 1999 2107 1037 2785 1997 1037 3185 2130 2065 1057 2123 1005 1056 2729 2055 1996 5436 2012 2560 2017 2215 2070 3835 2895 1012 1045 3053 2079 5422 2125 1999 1996 2690 1013 2364 2112 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.345009 140059782334272 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.346504 140059782334272 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.347896 140059782334272 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.353489 140059782334272 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.354552 140059782334272 tf_logging.py:115] guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] be careful with this one . once you get yer mit ##ts on it , it ' ll change the way you look at kung - fu flick ##s . you will be yearning a plot from all of the kung - fu films now , you will be wanting character depth and development , you will be craving mystery and un ##pre ##dict ##ability , you will demand dynamic camera work and incredible backdrop ##s . sadly , you won ' t find all of these aspects together in one kung - fu movie , except for five deadly venom ##s ! < br / > < br / > easily the best kung - fu movie of all - time , venom ##s blend [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.356034 140059782334272 tf_logging.py:115] tokens: [CLS] be careful with this one . once you get yer mit ##ts on it , it ' ll change the way you look at kung - fu flick ##s . you will be yearning a plot from all of the kung - fu films now , you will be wanting character depth and development , you will be craving mystery and un ##pre ##dict ##ability , you will demand dynamic camera work and incredible backdrop ##s . sadly , you won ' t find all of these aspects together in one kung - fu movie , except for five deadly venom ##s ! < br / > < br / > easily the best kung - fu movie of all - time , venom ##s blend [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2022 6176 2007 2023 2028 1012 2320 2017 2131 20416 10210 3215 2006 2009 1010 2009 1005 2222 2689 1996 2126 2017 2298 2012 18577 1011 11865 17312 2015 1012 2017 2097 2022 29479 1037 5436 2013 2035 1997 1996 18577 1011 11865 3152 2085 1010 2017 2097 2022 5782 2839 5995 1998 2458 1010 2017 2097 2022 26369 6547 1998 4895 28139 29201 8010 1010 2017 2097 5157 8790 4950 2147 1998 9788 18876 2015 1012 13718 1010 2017 2180 1005 1056 2424 2035 1997 2122 5919 2362 1999 2028 18577 1011 11865 3185 1010 3272 2005 2274 9252 15779 2015 999 1026 7987 1013 1028 1026 7987 1013 1028 4089 1996 2190 18577 1011 11865 3185 1997 2035 1011 2051 1010 15779 2015 12586 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.356955 140059782334272 tf_logging.py:115] input_ids: 101 2022 6176 2007 2023 2028 1012 2320 2017 2131 20416 10210 3215 2006 2009 1010 2009 1005 2222 2689 1996 2126 2017 2298 2012 18577 1011 11865 17312 2015 1012 2017 2097 2022 29479 1037 5436 2013 2035 1997 1996 18577 1011 11865 3152 2085 1010 2017 2097 2022 5782 2839 5995 1998 2458 1010 2017 2097 2022 26369 6547 1998 4895 28139 29201 8010 1010 2017 2097 5157 8790 4950 2147 1998 9788 18876 2015 1012 13718 1010 2017 2180 1005 1056 2424 2035 1997 2122 5919 2362 1999 2028 18577 1011 11865 3185 1010 3272 2005 2274 9252 15779 2015 999 1026 7987 1013 1028 1026 7987 1013 1028 4089 1996 2190 18577 1011 11865 3185 1997 2035 1011 2051 1010 15779 2015 12586 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.358808 140059782334272 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.360312 140059782334272 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.361347 140059782334272 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.366170 140059782334272 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.367263 140059782334272 tf_logging.py:115] guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] chili palmer is tired of doing movies and know wants to do some success in music . being half ma ##fi ##oso half expert ne ##go ##tia ##tor , he wants to rise in the music market . however , know everyone is like him and making the good singer linda moon to record a hit will be harder as expected . the first part is funny and filled with irony , this one falls into the easy jokes and has many less good moments . only the two women ( um ##a and mil ##ian ) are decent in their part . most of the film is done with his ##tri ##onic character , excessive ##ly exaggerated ( a little is ok , too much [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.368298 140059782334272 tf_logging.py:115] tokens: [CLS] chili palmer is tired of doing movies and know wants to do some success in music . being half ma ##fi ##oso half expert ne ##go ##tia ##tor , he wants to rise in the music market . however , know everyone is like him and making the good singer linda moon to record a hit will be harder as expected . the first part is funny and filled with irony , this one falls into the easy jokes and has many less good moments . only the two women ( um ##a and mil ##ian ) are decent in their part . most of the film is done with his ##tri ##onic character , excessive ##ly exaggerated ( a little is ok , too much [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 20238 8809 2003 5458 1997 2725 5691 1998 2113 4122 2000 2079 2070 3112 1999 2189 1012 2108 2431 5003 8873 19137 2431 6739 11265 3995 10711 4263 1010 2002 4122 2000 4125 1999 1996 2189 3006 1012 2174 1010 2113 3071 2003 2066 2032 1998 2437 1996 2204 3220 8507 4231 2000 2501 1037 2718 2097 2022 6211 2004 3517 1012 1996 2034 2112 2003 6057 1998 3561 2007 19728 1010 2023 2028 4212 2046 1996 3733 13198 1998 2038 2116 2625 2204 5312 1012 2069 1996 2048 2308 1006 8529 2050 1998 23689 2937 1007 2024 11519 1999 2037 2112 1012 2087 1997 1996 2143 2003 2589 2007 2010 18886 12356 2839 1010 11664 2135 16903 1006 1037 2210 2003 7929 1010 2205 2172 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.369133 140059782334272 tf_logging.py:115] input_ids: 101 20238 8809 2003 5458 1997 2725 5691 1998 2113 4122 2000 2079 2070 3112 1999 2189 1012 2108 2431 5003 8873 19137 2431 6739 11265 3995 10711 4263 1010 2002 4122 2000 4125 1999 1996 2189 3006 1012 2174 1010 2113 3071 2003 2066 2032 1998 2437 1996 2204 3220 8507 4231 2000 2501 1037 2718 2097 2022 6211 2004 3517 1012 1996 2034 2112 2003 6057 1998 3561 2007 19728 1010 2023 2028 4212 2046 1996 3733 13198 1998 2038 2116 2625 2204 5312 1012 2069 1996 2048 2308 1006 8529 2050 1998 23689 2937 1007 2024 11519 1999 2037 2112 1012 2087 1997 1996 2143 2003 2589 2007 2010 18886 12356 2839 1010 11664 2135 16903 1006 1037 2210 2003 7929 1010 2205 2172 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.371129 140059782334272 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.372383 140059782334272 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.373336 140059782334272 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.386667 140059782334272 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.388248 140059782334272 tf_logging.py:115] guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] following is a little - known 1998 british film , which was made with a budget of £ ##80 ##00 and has a running time of 70 minutes . when watching it , you ' d never expect its director to go on to make it in hollywood and become one of the most acclaimed and celebrated directors of the 21st century well , everybody has to start somewhere i suppose . < br / > < br / > the director of following , as you probably already know is englishman christopher nolan , who directly after following would go on to direct the critically - acclaimed independent film me ##mento ; a few years later he would be hired by warner bros . to [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.389198 140059782334272 tf_logging.py:115] tokens: [CLS] following is a little - known 1998 british film , which was made with a budget of £ ##80 ##00 and has a running time of 70 minutes . when watching it , you ' d never expect its director to go on to make it in hollywood and become one of the most acclaimed and celebrated directors of the 21st century well , everybody has to start somewhere i suppose . < br / > < br / > the director of following , as you probably already know is englishman christopher nolan , who directly after following would go on to direct the critically - acclaimed independent film me ##mento ; a few years later he would be hired by warner bros . to [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2206 2003 1037 2210 1011 2124 2687 2329 2143 1010 2029 2001 2081 2007 1037 5166 1997 1069 17914 8889 1998 2038 1037 2770 2051 1997 3963 2781 1012 2043 3666 2009 1010 2017 1005 1040 2196 5987 2049 2472 2000 2175 2006 2000 2191 2009 1999 5365 1998 2468 2028 1997 1996 2087 10251 1998 6334 5501 1997 1996 7398 2301 2092 1010 7955 2038 2000 2707 4873 1045 6814 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2472 1997 2206 1010 2004 2017 2763 2525 2113 2003 25244 5696 13401 1010 2040 3495 2044 2206 2052 2175 2006 2000 3622 1996 11321 1011 10251 2981 2143 2033 23065 1025 1037 2261 2086 2101 2002 2052 2022 5086 2011 6654 10243 1012 2000 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.390362 140059782334272 tf_logging.py:115] input_ids: 101 2206 2003 1037 2210 1011 2124 2687 2329 2143 1010 2029 2001 2081 2007 1037 5166 1997 1069 17914 8889 1998 2038 1037 2770 2051 1997 3963 2781 1012 2043 3666 2009 1010 2017 1005 1040 2196 5987 2049 2472 2000 2175 2006 2000 2191 2009 1999 5365 1998 2468 2028 1997 1996 2087 10251 1998 6334 5501 1997 1996 7398 2301 2092 1010 7955 2038 2000 2707 4873 1045 6814 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2472 1997 2206 1010 2004 2017 2763 2525 2113 2003 25244 5696 13401 1010 2040 3495 2044 2206 2052 2175 2006 2000 3622 1996 11321 1011 10251 2981 2143 2033 23065 1025 1037 2261 2086 2101 2002 2052 2022 5086 2011 6654 10243 1012 2000 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.391484 140059782334272 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.392612 140059782334272 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.393316 140059782334272 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.396054 140059782334272 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.396872 140059782334272 tf_logging.py:115] guid: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] dark angel is a cross between hu ##xley ' s brave new world and percy ' s love in the ruins - - portraying the not too distant future as a disturbing mixture of chaos and order , both in the worst sense of the word . once one swallows the premise that all modern technology can be brought to a stands ##till by \" the pulse , \" it provides an entertaining landscape for exploring the personalities of and relationships between the two primary characters - - max ( the dark angel / bike messenger ) and logan ( the rich rebel ) . it seems uneven , perhaps a result of a variety of authors , but is held together by the energetic , [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.397887 140059782334272 tf_logging.py:115] tokens: [CLS] dark angel is a cross between hu ##xley ' s brave new world and percy ' s love in the ruins - - portraying the not too distant future as a disturbing mixture of chaos and order , both in the worst sense of the word . once one swallows the premise that all modern technology can be brought to a stands ##till by \" the pulse , \" it provides an entertaining landscape for exploring the personalities of and relationships between the two primary characters - - max ( the dark angel / bike messenger ) and logan ( the rich rebel ) . it seems uneven , perhaps a result of a variety of authors , but is held together by the energetic , [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2601 4850 2003 1037 2892 2090 15876 20959 1005 1055 9191 2047 2088 1998 11312 1005 1055 2293 1999 1996 8435 1011 1011 17274 1996 2025 2205 6802 2925 2004 1037 14888 8150 1997 8488 1998 2344 1010 2119 1999 1996 5409 3168 1997 1996 2773 1012 2320 2028 26436 1996 18458 2008 2035 2715 2974 2064 2022 2716 2000 1037 4832 28345 2011 1000 1996 8187 1010 1000 2009 3640 2019 14036 5957 2005 11131 1996 12857 1997 1998 6550 2090 1996 2048 3078 3494 1011 1011 4098 1006 1996 2601 4850 1013 7997 11981 1007 1998 6307 1006 1996 4138 8443 1007 1012 2009 3849 17837 1010 3383 1037 2765 1997 1037 3528 1997 6048 1010 2021 2003 2218 2362 2011 1996 18114 1010 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.400209 140059782334272 tf_logging.py:115] input_ids: 101 2601 4850 2003 1037 2892 2090 15876 20959 1005 1055 9191 2047 2088 1998 11312 1005 1055 2293 1999 1996 8435 1011 1011 17274 1996 2025 2205 6802 2925 2004 1037 14888 8150 1997 8488 1998 2344 1010 2119 1999 1996 5409 3168 1997 1996 2773 1012 2320 2028 26436 1996 18458 2008 2035 2715 2974 2064 2022 2716 2000 1037 4832 28345 2011 1000 1996 8187 1010 1000 2009 3640 2019 14036 5957 2005 11131 1996 12857 1997 1998 6550 2090 1996 2048 3078 3494 1011 1011 4098 1006 1996 2601 4850 1013 7997 11981 1007 1998 6307 1006 1996 4138 8443 1007 1012 2009 3849 17837 1010 3383 1037 2765 1997 1037 3528 1997 6048 1010 2021 2003 2218 2362 2011 1996 18114 1010 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.401564 140059782334272 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.402629 140059782334272 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:21.403728 140059782334272 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:04:57.783702 140059782334272 tf_logging.py:115] Writing example 10000 of 15000\n"
     ]
    }
   ],
   "source": [
    "test_examples = build_bert_pred_input_examples(test_documents)\n",
    "\n",
    "test_features = build_bert_pred_input_features(inp_examples=test_examples, label_list=label_list, \n",
    "                                               max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
    "\n",
    "predict_input_fn = build_bert_pred_input_function(inp_features=test_features, \n",
    "                                                  max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpj0glu3a3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 13:06:31.781743 140059782334272 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpj0glu3a3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpj0glu3a3', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f61a3754f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:31.784746 140059782334272 tf_logging.py:115] Using config: {'_model_dir': '/tmp/tmpj0glu3a3', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f61a3754f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:38.407441 140059782334272 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:41.834465 140059782334272 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:41.955253 140059782334272 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:42.453446 140059782334272 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_models/sentiment_v1/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:42.459108 140059782334272 tf_logging.py:115] Restoring parameters from ./tf_models/sentiment_v1/model.ckpt-5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:43.464135 140059782334272 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0621 13:06:43.533412 140059782334272 tf_logging.py:115] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config,\n",
    "                                   params={\"batch_size\": BATCH_SIZE})\n",
    "predictions = [pred for pred in estimator.predict(predict_input_fn, \n",
    "                                                  checkpoint_path=\"./tf_models/sentiment_v1/model.ckpt-5000\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'probabilities': array([-6.6754976e-05, -9.6146030e+00], dtype=float32),\n",
       "  'labels': 0},\n",
       " {'probabilities': array([-9.754098e+00, -5.805324e-05], dtype=float32),\n",
       "  'labels': 1},\n",
       " {'probabilities': array([-4.17231649e-06, -1.23868885e+01], dtype=float32),\n",
       "  'labels': 0},\n",
       " {'probabilities': array([-5.4796185e+00, -4.1796714e-03], dtype=float32),\n",
       "  'labels': 1},\n",
       " {'probabilities': array([-1.16084795e+01, -9.05986508e-06], dtype=float32),\n",
       "  'labels': 1}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([9.9993324e-01, 6.6746878e-05], dtype=float32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]['labels'], np.exp(predictions[0]['probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      7490\n",
      "           1       0.88      0.90      0.89      7510\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_predictions = [pred['labels'] for pred in predictions]\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true=test_labels, y_pred=test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm8V1W9//HXmwMGDggoKoGKKIp5zYkUr2YqhmIqmuZwNdG8P3JqsLSw682pci6z0sApHFIRr0NmGqE4kzOggICggqhkKKACwjmf3x97AUc8w/fA2ef7Pdv3s8d+nL3XXnuv9ZXT57vO2mutrYjAzMxavzblroCZmTUPB3Qzs4JwQDczKwgHdDOzgnBANzMrCAd0M7OCcEA3MysIB3Qzs4JwQDczK4i25a5AfT6ZNd5TWO0z1t/6kHJXwSrQokVvaE3vsfS9GSXHnHYb9lrj8vLgFrqZWUFUbAvdzKxF1VSXuwZrzAHdzAygelm5a7DGHNDNzICImnJXYY05oJuZAdQ4oJuZFYNb6GZmBeGHomZmBeEWuplZMYRHuZiZFYQfipqZFYS7XMzMCsIPRc3MCsItdDOzgvBDUTOzgvBDUTOzYohwH7qZWTG4D93MrCDc5WJmVhBuoZuZFUT10nLXYI05oJuZgbtczMwKowBdLm3KXQEzs4pQU1P61ghJnSSNkjRF0mRJu0vqImm0pGnpZ+eUV5KukjRd0gRJO9e6z+CUf5qkwY2V64BuZgbNGtCB3wIPRkQfYAdgMjAUGBMRvYEx6RhgINA7bUOAawAkdQHOBXYDdgXOXf4lUB8HdDMzIKqXlrw1RFJHYC/geoCI+CQiPgAGASNSthHAoWl/EHBTZMYBnSR1A/YHRkfEvIh4HxgNHNBQ2Q7oZmaQ9aGXujWsF/Av4EZJL0q6TtI6wMYR8TZA+rlRyt8dmFXr+tkprb70ejmgm5lBk7pcJA2R9FytbUitO7UFdgauiYidgI9Y2b1SF9WRFg2k18ujXMzMoEmjXCJiODC8ntOzgdkR8c90PIosoL8rqVtEvJ26VObWyr9pret7AHNS+t6rpI9tqF5uoZuZQbM9FI2Id4BZkrZJSf2BScB9wPKRKoOBe9P+fcDxabRLP2B+6pJ5CBggqXN6GDogpdXLLXQzM2jucejfA26VtBYwAziRrAE9UtJJwJvAt1LeB4ADgenAxykvETFP0oXAsynfBRExr6FCHdDNzACWNd8LLiLiJaBvHaf615E3gNPquc8NwA2lluuAbmYGhZgp6oBuZgZey8XMrDDcQjczKwi30M3MCsItdDOzgmjGUS7l4oBuZgYQDc6qbxUc0M3MwH3oZmaF4YBuZlYQfihqZlYQ1dXlrsEac0A3MwN3uZiZFYYDuplZQbgP3cysGKLG49DNzIrBXS5mZgXhUS5mZgXhFrqtrgUffsR5V/yRaa/PQhIXnHkKTz37Enc9MIbOnToC8P3vHMNeu+3M0qXLOP/K4bzy6mu0adOGoaeewFd23A6Aq264jftGP8aChR/yzP03l/MjWTP73vdO4oQTjiYieOWVKQwZchb9+u3CRRf9D2ut1Y4XX5zIySf/hOrqajp16siwYZexxRabs2TJEr773bOYNGlquT9C61KAgN6m3BX4vLrkDzeyx1d25C83Xsldwy6j12bdAfj24d9g1LDLGDXsMvbabWcARj3wDwDuvu4Khl9yDpcNu4ma9Mv3tX67cNvvf1WeD2G5+eIXN+bUU09kjz0Oom/fAVRVVXHUUYO47rorOP740+nbdwBvvvkWxx13BAA/+cnpjB8/iV13PYCTTvoRl19+Xnk/QGsUUfpWoRzQy+DDjz7m+YmT+ebAfQFo164tHdddp978r70xm912+g8ANui8Ph3XXYdXps4AYIcvbU3XDTrnX2lrcW3bVtGhQ3uqqqro0KEDH3/8MUuWfML06TMBePjhxzn00IEA9OnTm7FjnwRg6tTX2HzzHmy00YZlq3urVFNT+lahcg/okjpI2ibvclqT2W/PpfP6HTnnsqv51nd/wrlX/JGPFy0G4LZ7H+Kb/+9M/veyq5m/8EMAtunVk0eeeo5l1dXMfnsuk6bO4J2575XzI1jO5sx5lyuvHM7UqU8zc+azLFiwkFGj7qddu7bsvPP2ABx22IH06NENgIkTJzFoUBbc+/bdgc0260737puUrf6tUk2UvlWoXAO6pIOBl4AH0/GOku7Ls8zWoLq6msnTZnLUwQO4c9ildGj/Ba6//R6OPGQAD9z0O0YNu5SuG3Tm8j/eBMBhA/dh4w27cPSpQ7nk6j+xw3bbUFVVVeZPYXnq1KkjBx00gG233ZNevXZlnXU6cPTRh3H88d/j0kt/zuOP38vChR+xbFk2MuPyy6+hU6eOjBv3AKeccgLjx7+y4pyVqLq69K1C5f1Q9DxgV2AsQES8JKlnfZklDQGGAPzhonP472OPyLl65bFx1w3YuOsGfHnb3gB8fa9+XH/bPWzYudOKPIcf2J/Tz7kEgLZVVfz01BNWnDvu++eweWqZWTHtu++evP76LN57bx4A99zzIP367cLtt9/Nfvt9C4D+/b9K795bALBw4Yd897tnrbh+ypQneP31WS1f8VYsKrgrpVR5d7ksi4j5pWaOiOER0Tci+hY1mANs2KUTm3TdgJmz5gDwzxcmsuXmPfjXv99fkWfME8+wVc9NAVi0eMmKLpmnnp9AVVUVW27eo+Urbi1m1qw57LrrTnTo0B6AffbZg1dfnU7XrhsAsNZaa/HjH5/CtdfeCsD663ekXbt2AJx44tE88cQzLExddlaiAnS55N1Cf1nSfwFVknoD3weeyrnMVuHs07/D0IuuYunSZfTothEXnnUqF//hRqZMfx1JdN+kKz//4RAA5n0wn5OH/hK1acNGG3ThoqGnr7jPr4ffwl8ffoLFSz6h/9Enc/jAfTl18JHl+ljWTJ599iXuvvsBnn76ryxbVs348a9w/fV/5rzzzmTgwP60aSOuvfYWHn00+79Tnz5bcd11v6a6upopU6Zz8slnNVKCfUYB1nJR5DgER9LawP8AA1LSQ8AvImJxY9d+Mmt85X4NWtmsv/Uh5a6CVaBFi97Qmt7jowuOLTnmrPPzW9e4vDzk3ULfJiL+hyyom5lVrgI8RM67D/3XkqZIulDSdjmXZWa2+qKm9K0Rkl6XNFHSS5KeS2ldJI2WNC397JzSJekqSdMlTZC0c637DE75p0ka3Fi5uQb0iNgH2Bv4FzA8fcBz8izTzGy1NP9D0X0iYseI6JuOhwJjIqI3MCYdAwwEeqdtCHANZF8AwLnAbmSjBc9d/iVQn9wnFkXEOxFxFXAy2Zj0n+ddpplZU0VNTcnbahoEjEj7I4BDa6XfFJlxQCdJ3YD9gdERMS8i3gdGAwc0VEDeE4u2lXSepJeB35ONcPF4OzOrPM3bQg/g75KeT/NrADaOiLcB0s+NUnp3oPakgdkprb70euX9UPRG4DZgQETMybksM7PV14Tx5bUnQSbDI2J4reM9ImKOpI2A0ZKmNHS7OtKigfR65RrQI6Jfnvc3M2s2TZjSn4L38AbOz0k/50q6m6wP/F1J3SLi7dSlMjdlnw1sWuvyHsCclL73KuljG6pXLl0ukkamnxPTU9vl20RJE/Io08xsTURNlLw1RNI6ktZbvk82D+dl4D5g+UiVwcC9af8+4Pg02qUfMD91yTwEDJDUOT0MHZDS6pVXC/0H6edBOd3fzKx5Nd+U/o2BuyVBFmP/HBEPSnoWGCnpJOBN4Fsp/wPAgcB04GPgRICImCfpQuDZlO+CiJjXUMG5BPTlHf/AqRHx09rnJF0C/PSzV5mZlVEzLc4VETOAHepI/zfQv470AE6r5143ADeUWnbewxa/XkfawJzLNDNrOi/OVTdJpwCnAr1W6TNfD3gyjzLNzNZIBQfqUuXVh/5n4G/ARaycDQWwsLE+IDOzcojq1r/aYl596POB+cAxAGksZntgXUnrRsSbeZRrZrba3EJvWHoF3a+BL5KNudwcmAx4oS4zqyiNDUdsDfJ+KPoLoB8wNSK2IHvC6z50M6s8BXgomndAX5qG6rSR1CYiHgF2zLlMM7Omq2nCVqHyXsvlA0nrAo8Bt0qaCyzLuUwzsyaLZRUcqUuUdwt9ELAIOAN4EHgNODjnMs3Mms4t9IZFxEe1DkfUm9HMrMyK8FA071EuC/nsco/zgeeAH6cpsmZm5VfBLe9S5d2H/muyZSD/TLa279HAJsCrZOsT7J1z+WZmJSlCCz3vPvQDImJYRCyMiAVpDeEDI+IOoMF345mZtagC9KHnHdBrJB0pqU3ajqx1rvV/HZpZYcSy0rdKlXdAPxb4Ntks0XfT/nGSOgCn51y2mVnJoqb0rVLlPcplBvUPU3wiz7LNzJqkggN1qXJtoUvaWtIYSS+n4y9LOifPMs3MVkcRWuh5d7lcC5wNLAWIiAlkI13MzCpKEQJ63sMW146IZ9K79Zar4EcKZvZ5FdVqPFOFyzugvydpS9KIFklHAG83fImZWcur5JZ3qfIO6KcBw4E+kt4CZpKNfDEzqyhR4xZ6Y94CbgQeAboAC4DBwAU5l2tm1iRuoTfuXuAD4AWyJQDMzCpShFvojekREQfkXIaZ2RorQgu90WGLktaR1Cbtby3pEEntSrz/U5K2X6Mampm1gJpqlbxVqlJa6I8BX5XUGRhDtvTtUZT2cHNP4ARJM4ElZCsuRkR8eTXra2aWi8/LQ1FFxMeSTgJ+FxGXSnqxxPsPXIO6mZm1mCIE9FJmikrS7mQt8r+mtJL63iPijbq21a2smVleIkrfSiGpStKLku5Px1tI+qekaZLukLRWSv9COp6ezvesdY+zU/qrkvZvrMxSAvoPyabv3x0Rr0jqRTYM0cysMKJGJW8l+gEwudbxJcBvIqI38D5wUko/CXg/IrYCfpPyIelLZEulbAccAFwtqaqhAhsN6BHxaEQcEhGXpOMZEfH9Uj+RmVlrEKGSt8ZI6gF8A7guHQvYFxiVsowADk37g1j5zuVRQP+UfxBwe0QsiYiZwHRg14bKrbfrRNJfaOAlFBFxSCOfycys1ahu3tErVwI/AdZLxxsAH0SseD3GbKB72u8OzAKIiGWS5qf83YFxte5Z+5o6NdQXfnlTam9m1po1ZWKRpCHAkFpJw9MrNpF0EDA3Ip6XtPfyS+oqspFzDV1Tp3oDekQ82tCFZmZF0pRRLil4D6/n9B7AIZIOBNoDHcla7J0ktU2t9B6snD0/G9gUmC2pLbA+MK9W+nK1r6lTKROLeksaJWmSpBnLt8auMzNrTZprlEtEnB0RPSKiJ9lDzYcj4liywSRHpGyDyZZGAbgvHZPOPxwRkdKPTqNgtgB6A880VHYpww9vBM4le/q6D3Aidf8pYGbWarXAOPSfArdL+gXwInB9Sr8euFnSdLKW+dEAaVThSGAS2XskTouI6oYKUDTydSPp+YjYRdLEiNg+pT0eEV9dgw/WqE9mjS9xtKd9nqy/tZ/F22ctWvTGGkfjiVscXHLM2X7mXyqyUVtKC31xWstlmqTTyZbE3SjfapmZtaxSJwxVslInFq0NfB/YBfg2K/t7zMwKoSZU8lapGm2hR8SzafdDsv5zM7PC+Vyshy7pEeoY+xgR++ZSIzOzMihCl0spfehn1tpvDxxO9sQ1V2tveWDeRVgrtGjO4+WughVUJXellKqULpfnV0l6UpInHZlZoVTXlPJIsbKV0uXSpdZhG7IHo5vkViMzszIoQI9LSV0uz7NyXYFlwExWLvtoZlYIn4suF2DbiFhcO0HSF3Kqj5lZWRRhlEspnUZP1ZH2dHNXxMysnGqasFWqhtZD34Rs7d0OknZi5fotHckmGpmZFUYUYImqhrpc9gdOIFuy8QpWBvQFwM/yrZaZWctaVoAul4bWQx8BjJB0eETc1YJ1MjNrcUVooZfSh76LpE7LDyR1Tss/mpkVRhH60EsJ6AMj4oPlBxHxPuBpnGZWKIFK3ipVKcMWqyR9ISKWAEjqAHjYopkVSiW3vEtVSkC/BRgj6cZ0fCIwIr8qmZm1vOoKbnmXqpS1XC6VNAHYj2yky4PA5nlXzMysJeX/Brr8ldJCB3iH7C+SI8mm/nvUi5kVSk2RW+iStiZ7WekxwL+BO8jeQbpPC9XNzKzFFH1xrinA48DBETEdQNIZLVIrM7MWVoSHog0NWzycrKvlEUnXSuoPBfibxMysDjVSyVulqjegR8TdEXEU0AcYC5wBbCzpGkkDWqh+ZmYtoroJW6VqdGJRRHwUEbdGxEFk67q8BAzNvWZmZi2oRqVvlapJ71yKiHkRMcwviDazoqlBJW+VqtRhi2ZmhVb0US5mZp8bldyVUioHdDMzij9s0czsc6NapW8NkdRe0jOSxkt6RdL5KX0LSf+UNE3SHZLWSulfSMfT0/mete51dkp/VdL+jX0GB3QzM5p1PfQlwL4RsQOwI3CApH7AJcBvIqI38D5wUsp/EvB+RGwF/CblQ9KXyGbrbwccAFwtqaqhgh3QzcxovoAemQ/TYbu0BbAvMCqljwAOTfuDWLmC7SigvySl9NsjYklEzASmA7s2VLYDupkZECp9a4ykKkkvAXOB0cBrwAcRsSxlmQ10T/vdgVkA6fx8YIPa6XVcUycHdDMzmtZClzRE0nO1tiG17xUR1RGxI9lkzF2BbesocvlIybq+IqKB9Hp5lIuZGU2b0h8Rw4HhJeT7QNJYoB/QSVLb1ArvAcxJ2WYDmwKzJbUF1gfm1UpfrvY1dXIL3cyM5pv6L6mrpE5pvwPZy4EmA48AR6Rsg4F70/596Zh0/uGIiJR+dBoFswXQG3imobLdQjczo1nHoXcDRqQRKW2AkRFxv6RJwO2SfgG8CFyf8l8P3CxpOlnL/GiAiHhF0khgErAMOC0iGvxDwgHdzIzmC+gRMQHYqY70GdQxSiUiFgPfqudevwR+WWrZDuhmZngtFzOzwvBaLmZmBVHJL64olQO6mRlQU4BOFwd0MzOKsdqiA7qZGX4oamZWGG6hm5kVxDK1/ja6A7qZGe5yMTMrDHe5mJkVhIctmpkVROsP5w7oZmaAu1zMzAqjugBtdAd0MzPcQjczK4xwC93MrBjcQrc1tvXWW/LnW69Zcdxri8047/zL6ddvF7beeksAOq3fkQ/mL6DvVwYAsP3223LNHy5hvY7rUlNTQ7/dv8GSJUvKUn9rXgsWfsi5F1/J9BlvgMSFPzuDm++4h9ffnA3Awg8/ZL111+WuEX9g4qRXOe+Sq4CsdXnqd45lv6/tseJe1dXVHHXS99mo64Zcfdn5Zfk8rYmHLdoamzr1tRWBuk2bNrz5+vPcc+/fuOp3163Ic9klP2f+ggUAVFVVMeJPV3HCiT9gwoRJdOnSmaVLl5al7tb8Lr7yj+yxW19+88tzWLp0KYsWL+GKC89ecf6y313LuuusDcBWvTbnjuuvom3bKv713jwOH3wqe+/Rj7ZtqwC45c576dVzMz786OOyfJbWpvWH8+wFplYh+u+7JzNmvMGbb771qfQjjjiY2+/IXhA+4OtfY+LEyUyYMAmAefPep6amCH8s2ocffcTz41/m8IP3B6Bdu3Z0XG/dFecjggcffowDv743AB3at18RvJd88glo5St33pn7Lx576pkV97LGLSNK3ipVri10SQKOBXpFxAWSNgM2iYhn8iy3tTryyEHcfsc9n0r76p678e7cfzF9+kwAevfuRQQ8cP+tbNh1A0aOvJfLr7imrttZKzP7rXfo3Gl9zvnlr3l1+gy+tE1vhv7wZNbu0B6A58e/zAadO7P5pt1XXDPhlSn8769+w5x353LR/565IsBf8tth/OjUk/jo40Vl+SytUREeiubdQr8a2B04Jh0vBP5QX2ZJQyQ9J+m5mpqPcq5aZWnXrh0HHzSAUXfd/6n0o446lDtS6xygbdsq9vjPr/Dtwafztb0P5dBBA9l3nz1burqWg2XV1UyeOp2jDvsGo/70Bzp0aM/1N49ccf6B0WM58Otf+9Q1X96uD/feOozbr/st1908kiVLPmHsk/+kS+dObNend0t/hFatpglbpco7oO8WEacBiwEi4n1grfoyR8TwiOgbEX3btFkn56pVlgMO2IcXX5zI3LnvrUirqqrisEMHMvLO+1akzX7rbR57fBz//vf7LFq0mL89+DA77fQf5aiyNbNNNtqQjbtuyJe36wPAgL33ZNLU6QAsW1bNPx59igP671XntVv23IwO7dszbcbrvDhhEmOfGMeAwwdz1rkX88zz4/np+Ze22OdoraIJ/6tUeQf0pZKqSM8bJHWlsr/gyuboow79THfLfv2/yquvTuett95ekfb3vz/K9ttvS4cO7amqqmKvr/Zj8uRpLV1dy8GGG3Rhk426MvONbETLuOdfYsuem2X7z71Ir817sMlGXVfknz3nHZYty15tPOedd3n9zdl077YxZ5xyImPuuYW/3zWCy84fyq677MAl5/6k5T9QK1OEFnreo1yuAu4GNpL0S+AI4Jycy2x1OnRoz3799+KUU3/6qfSsT/3eT6V98MF8rvztcMY9/UD2kOzBh3ngb2NasrqWo5+dcQo/Pf9Sli5byqZf7MaFPzsDgL/941EG7rf3p/K+MOEVrr95JG3btqVNG3HOmafRudP6Zah1MVRH5ba8S6XI+UNI6gP0BwSMiYjJpVzXdq3urf+/rjW7RXMeL3cVrAK127CXGs/VsP/a/LCSY86f37h7jcvLQ96jXH4L3BER9T4INTOrBJXcN16qvPvQXwDOkTRd0mWS+uZcnpnZammuPnRJm0p6RNJkSa9I+kFK7yJptKRp6WfnlC5JV6U4OUHSzrXuNTjlnyZpcGOfIdeAHhEjIuJAYFdgKnCJJD/BM7OKU0OUvDViGfDjiNgW6AecJulLwFCybufewJh0DDAQ6J22IcA1kH0BAOcCu5HF0HOXfwnUp6Vmim4F9AF6AlNaqEwzs5I117DFiHg7Il5I+wuByUB3YBAwImUbARya9gcBN0VmHNBJUjdgf2B0RMxLQ75HAwc0VHbefeiXAN8EXgNGAhdGxAd5lmlmtjryGOUiqSewE/BPYOOIeBuyoC9po5StOzCr1mWzU1p96fXKe9jiTGD3iHiv0ZxmZmXUlNUWJQ0h6x5ZbnhEDF8lz7rAXcAPI2KBVO/AmLpORAPp9coloEvqExFTgGeAzdIaLitrlP4cMTOrFE2ZMJSC9/D6zktqRxbMb42I/0vJ70rqllrn3YC5KX02sGmty3sAc1L63qukj22oXnm10H9E9u11RR3nAtg3p3LNzFZLcw1bTIsSXg9Mjohf1zp1HzAYuDj9vLdW+umSbid7ADo/Bf2HgF/VehA6ADibBuQS0CNi+Z8iAyNice1zktrnUaaZ2Zpoxhdc7AF8G5go6aWU9jOyQD5S0knAm8C30rkHgAOB6cDHwIkAETFP0oXAsynfBRExr6GC8+5DfwrYuYQ0M7Oyaq5Z8xHxBHX3f0M2a37V/AGcVs+9bgBuKLXsvPrQNyF7GttB0k6s/HAdgbXzKNPMbE1UF2CmaF4t9P2BE8g68Wv3IS0k+9PDzKyi+J2i9YiIEcAISYdHxF15lGFm1pzyXqiwJeTV5XJcRNwC9JT0o1XPr/Lk18ys7NxCr9/y1w2t22AuM7MKUYTVFvPqchmWfp6fx/3NzJpbEV5wkeviXJIuldRRUjtJYyS9J+m4PMs0M1sdzbjaYtnkvdrigIhYABxENo11a+CsnMs0M2uyIgT0vCcWtUs/DwRuSzOfci7SzKzpPMqlcX+RNAVYBJwqqSuwuJFrzMxaXCW3vEuV9xuLhgK7A30jYinwEdli7mZmFaW5XnBRTnm/4KId2SI1e6WulkeBP+ZZppnZ6qiOpiygW5ny7nK5hqwf/ep0/O2U9t85l2tm1iTuQ2/cVyJih1rHD0san3OZZmZN5j70xlVL2nL5gaReQHXOZZqZNZn70Bt3FvCIpBnpuCdp8XYzs0pSU4Aul7xb6E8Cw8he11eT9p/OuUwzsyZzC71xNwELgAvT8THAzax89ZKZWUXwKJfGbbPKQ9FH/FDUzCqRu1wa96KkfssPJO1G1g1jZlZR3OXSuN2A4yW9mY43AyZLmkj2btQv51y+mVlJitBCzzugH5Dz/c3MmkUlt7xLlWtAj4g38ry/mVlzqY7WP0Um7xa6mVmr4Kn/ZmYFUYSp/w7oZma4hW5mVhge5WJmVhBFGOWS98QiM7NWoTpqSt4aI+kGSXMlvVwrrYuk0ZKmpZ+dU7okXSVpuqQJknaudc3glH+apMGNleuAbmZG1ode6laCP/HZeThDgTER0RsYk44BBgK90zaE7CVASOoCnEs2QXNX4NzlXwL1cUA3MyPrQy91a0xEPAbMWyV5EDAi7Y8ADq2VflNkxgGdJHUD9gdGR8S8iHgfGE0jkzXdh25mRouMctk4It5OZb0taaOU3h2YVSvf7JRWX3q93EI3MyMbh17qJmmIpOdqbUPWoGjVkRYNpNfLLXQzM5rWQo+I4cDwJhbxrqRuqXXeDZib0mcDm9bK1wOYk9L3XiV9bEMFuIVuZkbzjnKpx33A8pEqg4F7a6Ufn0a79APmp66Zh4ABkjqnh6EDUlq93EI3M6N5JxZJuo2sdb2hpNlko1UuBkZKOgl4k5VvbnsAOBCYDnxMeu9yRMyTdCHwbMp3QUSs+qD10+VW6nTXtmt1r8yKWVktmvN4uatgFajdhr3q6m9ukvbtNys55ixe/OYal5cHt9DNzCjGTFEHdDMzvDiXmVlhFGFxrortQ7eVJA1Jw6TMVvDvha3KwxZbhzWZtGDF5d8L+xQHdDOzgnBANzMrCAf01sH9pFYX/17Yp/ihqJlZQbiFbmZWEA7orYykTpJOrXX8RUmjylkna1mSTpZ0fNo/QdIXa527TtKXylc7Kyd3ubQyknoC90fEf5S5KlYBJI0FzoyI58pdFys/t9CbmaSekiZLulbSK5L+LqmDpC0lPSjpeUmPS+qT8m8paZykZyVdIOnDlL6upDGSXpA0UdKgVMTFwJaSXpJ0WSrv5XTNPyVtV6suYyXtImmd9NLaZyW9WOtatR8nAAAFS0lEQVRe1sLSv9cUSSPSC4FHSVpbUv/0bzMx/Vt9IeW/WNKklPfylHaepDMlHQH0BW5Nvw8d0r95X0mnSLq0VrknSPpd2j9O0jPpmmGSqsrx38Jy0JQXo3or6eWxPYFlwI7peCRwHNlLYXuntN2Ah9P+/cAxaf9k4MO03xbomPY3JFtaU+n+L69S3stp/wzg/LTfDZia9n8FHJf2OwFTgXXK/d/q87ilf68A9kjHNwDnkL1qbOuUdhPwQ6AL8Cor/5LulH6eR9Yqh+yFB31r3X8sWZDvCkyvlf43YE9gW+AvQLuUfjVwfLn/u3hrns0t9HzMjIiX0v7zZP8n/k/gTkkvAcPIAi7A7sCdaf/Pte4h4FeSJgD/IHuX4MaNlDuSlWssH1nrvgOAoanssUB7YLMmfyprLrMi4sm0fwvQn+x3ZmpKGwHsBSwAFgPXSfom2VrZJYmIfwEzJPWTtAGwDfBkKmsX4Nn0+9Af6NUMn8kqgBfnyseSWvvVZIH4g4jYsQn3OJaslbVLRCyV9DpZIK5XRLwl6d+SvgwcBXw3nRJweES82oTyLT8lPbiKiGWSdiULukcDpwP7NqGcO8i+2KcAd0dESBIwIiLObmKdrRVwC71lLABmSvoWQHrV1A7p3Djg8LR/dK1r1gfmpmC+D7B5Sl8IrNdAWbcDPwHWj4iJKe0h4Hvp/8xI2mlNP5Ctkc0k7Z72jyH7C6ynpK1S2reBRyWtS/bv+ABZF0xdDYKGfh/+Dzg0lXFHShsDHLH8jfOSukjavJ7rrZVxQG85xwInSRoPvAIsfzD5Q+BHkp4h64aZn9JvBfpKei5dOwUgIv4NPCnpZUmX1VHOKLIvhpG10i4E2gET0gPUC5v1k1lTTQYGp+60LsBvyF47dqekiUAN8EeyQH1/yvco2TOSVf0J+OPyh6K1T0TE+8AkYPOIeCalTSLrs/97uu9oVnb/WSvnYYtlJmltYFH6c/hosgekHoVSUB52anlyH3r57QL8PnWHfAB8p8z1MbNWyi10M7OCcB+6mVlBOKCbmRWEA7qZWUE4oFuzk1SdhtG9LOnONJJnde+1t6T70/4hkoY2kPdTK1E2oYzzJJ25unU0qxQO6JaHRRGxYxqa9wnZGjUrpIlVTf7di4j7IuLiBrJ0Apoc0M2KwgHd8vY4sJVWrkJ5NfACsKmkAZKeTitK3plmRiLpgLQi4RPAN5ffKK0Y+Pu0v7GkuyWNT9t/sspKlCnfWWmVyQmSzq91r/+R9Kqkf5Ctc2LW6jmgW24ktQUGAsuXINgGuCkidgI+IpuxuF9E7Aw8RzZjtj1wLXAw8FVgk3pufxXwaETsAOxMNvt2KPBa+uvgLEkDgN7ArmTT5neRtJekXchm0+5E9oXxlWb+6GZl4YlFlocOaSU/yFro1wNfBN6IiHEpvR/wJbJlDADWAp4G+pCtPDgNQNItwJA6ytgXOB4gIqqB+ZI6r5JnQNpeTMfrkgX49cgWq/o4lXHfGn1aswrhgG55WLTqypIpaH9UOwkYHRHHrJJvR0pcjbAEAi6KiGGrlPHDZizDrGK4y8XKZRywx/IVBpW9tWdrskXItpC0Zcp3TD3XjwFOSddWSerIZ1cefAj4Tq2++e5plcHHgMPSG37WI+veMWv1HNCtLNILGE4Abkur/o0D+kTEYrIulr+mh6Jv1HOLHwD7pNUJnwe2W3Ulyoj4O9lLQ55O+UYB60XEC2TLyb4E3EXWLWTW6nktFzOzgnAL3cysIBzQzcwKwgHdzKwgHNDNzArCAd3MrCAc0M3MCsIB3cysIBzQzcwK4v8D0sTqRqz0ZVUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "with tf.Session() as session:\n",
    "    cm = tf.confusion_matrix(test_labels, test_predictions).eval()\n",
    "\n",
    "LABELS = ['negative', 'positive']\n",
    "sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS, fmt='g')\n",
    "xl = plt.xlabel(\"Predicted\")\n",
    "yl = plt.ylabel(\"Actuals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
